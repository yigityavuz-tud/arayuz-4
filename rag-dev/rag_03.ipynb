{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e33a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install uv\n",
    "# uv init\n",
    "# uv add unstructured\n",
    "# uv add \"unstructured[pdf]\"\n",
    "# uv pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e607a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.staging.base import elements_to_json\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "import requests\n",
    "\n",
    "# Standard imports\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.schema import Document\n",
    "from langchain.chains.query_constructor.schema import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "import lark\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba31f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "langchain_tracing_v2 = os.getenv('LANGCHAIN_TRACING_V2')\n",
    "langchain_endpoint = os.getenv('LANGCHAIN_ENDPOINT')\n",
    "langchain_api_key = os.getenv('LANGCHAIN_API_KEY')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "index_name = os.getenv('PINECONE_INDEX_NAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c93d98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enumerate_text_parts_with_position(text_elements, exclude_types=None, column_threshold=50):\n",
    "    \"\"\"\n",
    "    Enumerate consecutive parts of a page in reading order, assigning a 'position' field to each.\n",
    "    Footer always gets the last position in a page.\n",
    "    Returns a list of dicts, each with 'position', 'text', and other original fields.\n",
    "    \n",
    "    Args:\n",
    "        text_elements (list): List of text elements with coordinates and type\n",
    "        exclude_types (list): List of element types to exclude (except 'footer', which is handled specially)\n",
    "        column_threshold (int): Pixel threshold to determine column boundaries\n",
    "    \n",
    "    Returns:\n",
    "        list: List of dicts, each with 'position', 'text', and other original fields\n",
    "    \"\"\"\n",
    "    # Always process footers separately, regardless of exclude_types\n",
    "    if exclude_types is None:\n",
    "        exclude_types = []\n",
    "    exclude_types_no_footer = [t for t in exclude_types if t.lower() != 'footer']\n",
    "\n",
    "    # Split elements into footers and non-footers\n",
    "    footer_elements = [\n",
    "        elem for elem in text_elements\n",
    "        if elem.get('type', '').lower() == 'footer'\n",
    "    ]\n",
    "    non_footer_elements = [\n",
    "        elem for elem in text_elements\n",
    "        if elem.get('type', '').lower() not in [t.lower() for t in exclude_types_no_footer + ['footer']]\n",
    "    ]\n",
    "\n",
    "    # If there are no non-footer elements and no footers, return empty\n",
    "    if not non_footer_elements and not footer_elements:\n",
    "        return []\n",
    "\n",
    "    # Group non-footer elements by approximate Y position (rows)\n",
    "    rows = {}\n",
    "    for element in non_footer_elements:\n",
    "        coords = element.get('coordinates', [[0, 0]])\n",
    "        if coords:\n",
    "            x, y = coords[0]  # Upper-left corner\n",
    "            row_key = round(y / 20) * 20  # Group by 20-pixel rows\n",
    "            if row_key not in rows:\n",
    "                rows[row_key] = []\n",
    "            rows[row_key].append(element)\n",
    "\n",
    "    # Sort rows by Y position (top to bottom)\n",
    "    sorted_rows = sorted(rows.items())\n",
    "\n",
    "    # Flatten elements in reading order\n",
    "    ordered_elements = []\n",
    "    for row_y, row_elements in sorted_rows:\n",
    "        row_elements.sort(key=lambda elem: elem.get('coordinates', [[0, 0]])[0][0])\n",
    "        ordered_elements.extend(row_elements)\n",
    "\n",
    "    # Enumerate and assign position to non-footer elements\n",
    "    enumerated_parts = []\n",
    "    position = 0\n",
    "    for element in ordered_elements:\n",
    "        text = element.get('text', '').strip()\n",
    "        if text:\n",
    "            text = re.sub(r' +', ' ', text)\n",
    "            part = dict(element)\n",
    "            part['text'] = text\n",
    "            part['position'] = position\n",
    "            enumerated_parts.append(part)\n",
    "            position += 1\n",
    "\n",
    "    # Now process footers, assign them the last position(s)\n",
    "    for i, element in enumerate(footer_elements):\n",
    "        text = element.get('text', '').strip()\n",
    "        if text:\n",
    "            text = re.sub(r' +', ' ', text)\n",
    "            part = dict(element)\n",
    "            part['text'] = text\n",
    "            # If multiple footers, assign consecutive positions after non-footers\n",
    "            part['position'] = position + i\n",
    "            enumerated_parts.append(part)\n",
    "\n",
    "    return enumerated_parts\n",
    "\n",
    "\n",
    "def wrap_metadata(item):\n",
    "    \"\"\"Wrap all fields except 'element_id' and 'text' into a 'metadata' dict. \n",
    "    The values of metadata fields are the original values (including dicts).\"\"\"\n",
    "    if not isinstance(item, dict):\n",
    "        return item\n",
    "    new_item = {}\n",
    "    # Always keep 'element_id' and 'text' at top level if present\n",
    "    if \"element_id\" in item:\n",
    "        new_item[\"element_id\"] = item[\"element_id\"]\n",
    "    if \"text\" in item:\n",
    "        new_item[\"text\"] = item[\"text\"]\n",
    "    # Everything else goes into metadata\n",
    "    metadata = {}\n",
    "    for k, v in item.items():\n",
    "        if k not in (\"element_id\", \"text\", \"metadata\"):\n",
    "            metadata[k] = v\n",
    "        elif k == \"metadata\" and isinstance(v, dict):\n",
    "            # Merge existing metadata fields\n",
    "            for mk, mv in v.items():\n",
    "                metadata[mk] = mv\n",
    "    new_item[\"metadata\"] = metadata\n",
    "    return new_item\n",
    "\n",
    "def remove_metadata_fields(metadata, fields_to_remove):\n",
    "    \"\"\"Remove specified fields from a metadata dict.\"\"\"\n",
    "    if not isinstance(metadata, dict):\n",
    "        return metadata\n",
    "    for field in fields_to_remove:\n",
    "        metadata.pop(field, None)\n",
    "    return metadata\n",
    "\n",
    "def convert_languages_field_in_metadata(metadata):\n",
    "    \"\"\"If 'languages' in metadata is a list of length 1, convert it to a string.\"\"\"\n",
    "    if isinstance(metadata, dict) and \"languages\" in metadata:\n",
    "        if isinstance(metadata[\"languages\"], list) and len(metadata[\"languages\"]) == 1:\n",
    "            metadata[\"languages\"] = metadata[\"languages\"][0]\n",
    "    return metadata\n",
    "\n",
    "def get_position(x):\n",
    "    if isinstance(x, dict):\n",
    "        # Try top-level, then metadata\n",
    "        if \"position\" in x:\n",
    "            return x[\"position\"]\n",
    "        elif \"metadata\" in x and isinstance(x[\"metadata\"], dict) and \"position\" in x[\"metadata\"]:\n",
    "            return x[\"metadata\"][\"position\"]\n",
    "    return -1\n",
    "\n",
    "def get_type(x):\n",
    "    if isinstance(x, dict):\n",
    "        if \"type\" in x:\n",
    "            return x[\"type\"]\n",
    "        elif \"metadata\" in x and isinstance(x[\"metadata\"], dict) and \"type\" in x[\"metadata\"]:\n",
    "            return x[\"metadata\"][\"type\"]\n",
    "    return None\n",
    "\n",
    "def get_text(x):\n",
    "    if isinstance(x, dict):\n",
    "        return x.get(\"text\", None)\n",
    "    return None\n",
    "\n",
    "def replace_nulls(obj):\n",
    "    \"\"\"Recursively replace None values in dicts/lists with a space character.\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: replace_nulls(v) if v is not None else \" \" for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [replace_nulls(v) for v in obj]\n",
    "    else:\n",
    "        return obj if obj is not None else \" \"\n",
    "    \n",
    "def metadata_without_position(meta):\n",
    "    \"\"\"Return a copy of metadata dict without the 'position' field.\"\"\"\n",
    "    if not isinstance(meta, dict):\n",
    "        return meta\n",
    "    return {k: v for k, v in meta.items() if k != \"position\"}\n",
    "\n",
    "def can_merge_nodes(node1, node2):\n",
    "    \"\"\"Return True if node1 and node2 can be merged according to the rules.\"\"\"\n",
    "    if not (isinstance(node1, dict) and isinstance(node2, dict)):\n",
    "        return False\n",
    "    meta1 = node1.get(\"metadata\", {})\n",
    "    meta2 = node2.get(\"metadata\", {})\n",
    "    # Compare all metadata fields except 'position'\n",
    "    if metadata_without_position(meta1) != metadata_without_position(meta2):\n",
    "        return False\n",
    "    # Both must have 'text' fields that are strings\n",
    "    text1 = node1.get(\"text\", \"\")\n",
    "    text2 = node2.get(\"text\", \"\")\n",
    "    if not (isinstance(text1, str) and isinstance(text2, str)):\n",
    "        return False\n",
    "    # Total length of text fields must be less than 200\n",
    "    if len(text1) + len(text2) < 200:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def merge_nodes(node1, node2):\n",
    "    \"\"\"Merge node2 into node1, concatenating text and updating position to the lower of the two.\"\"\"\n",
    "    merged = dict(node1)\n",
    "    merged[\"text\"] = node1.get(\"text\", \"\") + \" \" + node2.get(\"text\", \"\")\n",
    "    # Merge metadata, keep the lower position\n",
    "    meta1 = node1.get(\"metadata\", {})\n",
    "    meta2 = node2.get(\"metadata\", {})\n",
    "    merged_meta = dict(meta1)\n",
    "    pos1 = meta1.get(\"position\", node1.get(\"position\", None))\n",
    "    pos2 = meta2.get(\"position\", node2.get(\"position\", None))\n",
    "    # Use the lower position if both exist\n",
    "    if pos1 is not None and pos2 is not None:\n",
    "        merged_meta[\"position\"] = min(pos1, pos2)\n",
    "    elif pos1 is not None:\n",
    "        merged_meta[\"position\"] = pos1\n",
    "    elif pos2 is not None:\n",
    "        merged_meta[\"position\"] = pos2\n",
    "    merged[\"metadata\"] = merged_meta\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39615c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: C:/Users/yigit/Desktop/Enterprises/arayuz-4/okumalar-pdf/30) TOG Gençlik Çalışmasının Toplumsal Katılıma Etkisi Araştırması.pdf -> C:/Users/yigit/Desktop/Enterprises/arayuz-4\\okumalar-json\\30) TOG Gençlik Çalışmasının Toplumsal Katılıma Etkisi Araştırması.json\n"
     ]
    }
   ],
   "source": [
    "pdf_dir = \"C:/Users/yigit/Desktop/Enterprises/arayuz-4/okumalar-pdf/\"\n",
    "json_dir = os.path.join(os.path.dirname(pdf_dir.rstrip(\"/\\\\\")), \"okumalar-json\")\n",
    "\n",
    "os.makedirs(json_dir, exist_ok=True)\n",
    "\n",
    "# Get all PDF and JSON base filenames (without extension)\n",
    "pdf_files = [f for f in os.listdir(pdf_dir) if f.lower().endswith(\".pdf\")]\n",
    "json_files = [f for f in os.listdir(json_dir) if f.lower().endswith('.json')]\n",
    "\n",
    "pdf_basenames = set(os.path.splitext(f)[0] for f in pdf_files)\n",
    "json_basenames = set(os.path.splitext(f)[0] for f in json_files)\n",
    "\n",
    "# Find PDFs that do not have a corresponding JSON\n",
    "unprocessed_basenames = pdf_basenames - json_basenames\n",
    "\n",
    "if not unprocessed_basenames:\n",
    "    print(\"All PDFs have been processed to JSON.\")\n",
    "else:\n",
    "    print(f\"Processing {len(unprocessed_basenames)} unprocessed PDFs...\")\n",
    "    for base_file_name in sorted(unprocessed_basenames):\n",
    "        pdf_path = os.path.join(pdf_dir, f\"{base_file_name}.pdf\")\n",
    "        json_path = os.path.join(json_dir, f\"{base_file_name}.json\")\n",
    "        print(f\"Processing: {pdf_path} -> {json_path}\")\n",
    "        elements = partition_pdf(\n",
    "            filename=pdf_path,\n",
    "            languages=[\"tur\"],\n",
    "            strategy=\"fast\",\n",
    "            infer_table_structure=True,\n",
    "        )\n",
    "        elements_to_json(elements=elements, filename=json_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd57c308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing JSON file: 1) Temel kavramlar önyargı, kalıpyargı ve ayrımcılık.json\n",
      "Number of nodes before split: 50\n",
      "Number of nodes after split: 138\n",
      "Saved processed JSON to: C:/Users/yigit/Desktop/Enterprises/arayuz-4\\okumalar-prep\\1) Temel kavramlar önyargı, kalıpyargı ve ayrımcılık.json\n",
      "\n",
      "Processing JSON file: 10) TÜRKİYE’DE ÖRGÜTLENME ÖZGÜRLÜĞÜNÜN GENEL GÖRÜNÜMÜ-II .json\n",
      "Number of nodes before split: 869\n",
      "Number of nodes after split: 1244\n",
      "Saved processed JSON to: C:/Users/yigit/Desktop/Enterprises/arayuz-4\\okumalar-prep\\10) TÜRKİYE’DE ÖRGÜTLENME ÖZGÜRLÜĞÜNÜN GENEL GÖRÜNÜMÜ-II .json\n",
      "\n",
      "Processing JSON file: 11) Yurttaslik_Alani_Bilgi_Notu_1.json\n",
      "Number of nodes before split: 263\n",
      "Number of nodes after split: 302\n",
      "Saved processed JSON to: C:/Users/yigit/Desktop/Enterprises/arayuz-4\\okumalar-prep\\11) Yurttaslik_Alani_Bilgi_Notu_1.json\n",
      "\n",
      "Processing JSON file: 12) TERÖRLE MÜCADELEYİ ARAÇSALLAŞTIRMAK.json\n",
      "Number of nodes before split: 210\n",
      "Number of nodes after split: 456\n",
      "Saved processed JSON to: C:/Users/yigit/Desktop/Enterprises/arayuz-4\\okumalar-prep\\12) TERÖRLE MÜCADELEYİ ARAÇSALLAŞTIRMAK.json\n",
      "\n",
      "Processing JSON file: 13) PROTESTO HAKKINI KORU.json\n",
      "Number of nodes before split: 386\n",
      "Number of nodes after split: 872\n",
      "Saved processed JSON to: C:/Users/yigit/Desktop/Enterprises/arayuz-4\\okumalar-prep\\13) PROTESTO HAKKINI KORU.json\n",
      "\n",
      "Processing JSON file: 14) KomploTeorileri_AR_23.03.23_web.json\n",
      "Number of nodes before split: 393\n",
      "Number of nodes after split: 539\n",
      "Saved processed JSON to: C:/Users/yigit/Desktop/Enterprises/arayuz-4\\okumalar-prep\\14) KomploTeorileri_AR_23.03.23_web.json\n",
      "\n",
      "Processing JSON file: 15) Feminist_Hareketin_Gundemleri_.json\n",
      "Number of nodes before split: 99\n",
      "Number of nodes after split: 230\n",
      "Saved processed JSON to: C:/Users/yigit/Desktop/Enterprises/arayuz-4\\okumalar-prep\\15) Feminist_Hareketin_Gundemleri_.json\n",
      "\n",
      "Processing JSON file: 16) Sivil Toplum Kuruluşlarının Devlet Tarafından Finansmanı Üzerine Bir Tartışma.json\n",
      "Number of nodes before split: 192\n",
      "Number of nodes after split: 263\n",
      "Saved processed JSON to: C:/Users/yigit/Desktop/Enterprises/arayuz-4\\okumalar-prep\\16) Sivil Toplum Kuruluşlarının Devlet Tarafından Finansmanı Üzerine Bir Tartışma.json\n",
      "\n",
      "Processing JSON file: 17) Gençlik Politikalarında Karşılaştırmalı Bir Değerlendirme-Türkiye ve Finlandiya Örneği.json\n",
      "Number of nodes before split: 680\n",
      "Number of nodes after split: 815\n",
      "Saved processed JSON to: C:/Users/yigit/Desktop/Enterprises/arayuz-4\\okumalar-prep\\17) Gençlik Politikalarında Karşılaştırmalı Bir Değerlendirme-Türkiye ve Finlandiya Örneği.json\n",
      "\n",
      "Processing JSON file: 18) Avrupa Konseyi Politik Karar Alma Süreçlerine Sivil Katılım Rehberi Çevirisi.json\n",
      "Number of nodes before split: 252\n",
      "Number of nodes after split: 285\n",
      "Saved processed JSON to: C:/Users/yigit/Desktop/Enterprises/arayuz-4\\okumalar-prep\\18) Avrupa Konseyi Politik Karar Alma Süreçlerine Sivil Katılım Rehberi Çevirisi.json\n",
      "\n",
      "Processing JSON file: 19) Kampüsten Öğrenci Toplulukları .json\n",
      "Number of nodes before split: 3478\n",
      "Number of nodes after split: 3485\n",
      "Saved processed JSON to: C:/Users/yigit/Desktop/Enterprises/arayuz-4\\okumalar-prep\\19) Kampüsten Öğrenci Toplulukları .json\n",
      "\n",
      "Processing JSON file: 2) Ayrımcılık ve medya.json\n",
      "Number of nodes before split: 108\n",
      "Number of nodes after split: 209\n",
      "Saved processed JSON to: C:/Users/yigit/Desktop/Enterprises/arayuz-4\\okumalar-prep\\2) Ayrımcılık ve medya.json\n",
      "\n",
      "Processing JSON file: 20) Gençler Ne(ler) İstiyor_ .json\n",
      "Number of nodes before split: 257\n",
      "Number of nodes after split: 258\n",
      "Saved processed JSON to: C:/Users/yigit/Desktop/Enterprises/arayuz-4\\okumalar-prep\\20) Gençler Ne(ler) İstiyor_ .json\n",
      "\n",
      "Processing JSON file: 21) Türkiye’de Gençlik ve Siyaset_ Gelecek İçin Nasıl Bir Katılım_ .json\n",
      "Number of nodes before split: 275\n",
      "Number of nodes after split: 429\n",
      "Saved processed JSON to: C:/Users/yigit/Desktop/Enterprises/arayuz-4\\okumalar-prep\\21) Türkiye’de Gençlik ve Siyaset_ Gelecek İçin Nasıl Bir Katılım_ .json\n",
      "\n",
      "Processing JSON file: 22) Gençlik Araştırmaları Dergisi 13.sayı.json\n",
      "Number of nodes before split: 5345\n",
      "Number of nodes after split: 5689\n",
      "Saved processed JSON to: C:/Users/yigit/Desktop/Enterprises/arayuz-4\\okumalar-prep\\22) Gençlik Araştırmaları Dergisi 13.sayı.json\n",
      "\n",
      "Processing JSON file: 23) Türkiye_de Gençlik Miti 1980 Sonrası Türkiye Gençliği İletişim Yayınları.json\n",
      "Number of nodes before split: 1935\n",
      "Number of nodes after split: 2870\n",
      "Saved processed JSON to: C:/Users/yigit/Desktop/Enterprises/arayuz-4\\okumalar-prep\\23) Türkiye_de Gençlik Miti 1980 Sonrası Türkiye Gençliği İletişim Yayınları.json\n",
      "\n",
      "Processing JSON file: 24) Türkiye’nin Gençliği Araştırması Raporu -SODEV- .json\n",
      "Number of nodes before split: 246\n",
      "Number of nodes after split: 246\n",
      "Saved processed JSON to: C:/Users/yigit/Desktop/Enterprises/arayuz-4\\okumalar-prep\\24) Türkiye’nin Gençliği Araştırması Raporu -SODEV- .json\n",
      "\n",
      "Processing JSON file: 25) Türkiye’de Gençlerin Güvencesizliği_ Çalışma, Geçim ve Yaşam Algısı.json\n",
      "Number of nodes before split: 848\n",
      "Number of nodes after split: 1112\n",
      "Saved processed JSON to: C:/Users/yigit/Desktop/Enterprises/arayuz-4\\okumalar-prep\\25) Türkiye’de Gençlerin Güvencesizliği_ Çalışma, Geçim ve Yaşam Algısı.json\n",
      "\n",
      "Processing JSON file: 26) Toplumun Boğaziçi Üniversitesi Olaylarına Bakışı.json\n",
      "Number of nodes before split: 368\n",
      "Number of nodes after split: 391\n",
      "Saved processed JSON to: C:/Users/yigit/Desktop/Enterprises/arayuz-4\\okumalar-prep\\26) Toplumun Boğaziçi Üniversitesi Olaylarına Bakışı.json\n",
      "\n",
      "Processing JSON file: 27) Kürt Gemçler’20 Benzerlikler Farklar Değişimler.json\n",
      "Number of nodes before split: 1917\n",
      "Number of nodes after split: 2089\n",
      "Saved processed JSON to: C:/Users/yigit/Desktop/Enterprises/arayuz-4\\okumalar-prep\\27) Kürt Gemçler’20 Benzerlikler Farklar Değişimler.json\n",
      "\n",
      "Processing JSON file: 28) NEET Gençler Araştırması – NEET Gençlerin İnsan Onuruna Yaraşır Yaşam Sürme Hakkına Erişimi.json\n",
      "Number of nodes before split: 811\n",
      "Number of nodes after split: 1369\n",
      "Saved processed JSON to: C:/Users/yigit/Desktop/Enterprises/arayuz-4\\okumalar-prep\\28) NEET Gençler Araştırması – NEET Gençlerin İnsan Onuruna Yaraşır Yaşam Sürme Hakkına Erişimi.json\n",
      "\n",
      "Processing JSON file: 29) TGSP Türkiye’nin Gençleri Araştırması.pdf.json\n",
      "Number of nodes before split: 0\n",
      "Number of nodes after split: 0\n",
      "Saved processed JSON to: C:/Users/yigit/Desktop/Enterprises/arayuz-4\\okumalar-prep\\29) TGSP Türkiye’nin Gençleri Araştırması.pdf.json\n",
      "\n",
      "Processing JSON file: 3) Toplumsal Cinsiyete Dayalı Ayrımcılık.json\n",
      "Number of nodes before split: 97\n",
      "Number of nodes after split: 159\n",
      "Saved processed JSON to: C:/Users/yigit/Desktop/Enterprises/arayuz-4\\okumalar-prep\\3) Toplumsal Cinsiyete Dayalı Ayrımcılık.json\n",
      "\n",
      "Processing JSON file: 30) TOG Gençlik Çalışmasının Toplumsal Katılıma Etkisi Araştırması.json\n",
      "Number of nodes before split: 1378\n",
      "Number of nodes after split: 1709\n",
      "Saved processed JSON to: C:/Users/yigit/Desktop/Enterprises/arayuz-4\\okumalar-prep\\30) TOG Gençlik Çalışmasının Toplumsal Katılıma Etkisi Araştırması.json\n",
      "\n",
      "Processing JSON file: 4) Uluslararası Af Örgütü Raporu 2021-2022 Avrupa ve Orta Asya Değerlendirmesi(sayfa 46-54).json\n",
      "Number of nodes before split: 112\n",
      "Number of nodes after split: 174\n",
      "Saved processed JSON to: C:/Users/yigit/Desktop/Enterprises/arayuz-4\\okumalar-prep\\4) Uluslararası Af Örgütü Raporu 2021-2022 Avrupa ve Orta Asya Değerlendirmesi(sayfa 46-54).json\n",
      "\n",
      "Processing JSON file: 5) Paralel Kariyer Arayışının Nedenleri Isparta’da Faaliyet Gösteren STK’larda Bir Araştırma.json\n",
      "Number of nodes before split: 292\n",
      "Number of nodes after split: 347\n",
      "Saved processed JSON to: C:/Users/yigit/Desktop/Enterprises/arayuz-4\\okumalar-prep\\5) Paralel Kariyer Arayışının Nedenleri Isparta’da Faaliyet Gösteren STK’larda Bir Araştırma.json\n",
      "\n",
      "Processing JSON file: 6) Sivil Toplum Örgütlerinde Profesyonel ve Gönüllü Çalışma İlişkileri Tehditler Ve Fırsatlar.json\n",
      "Number of nodes before split: 155\n",
      "Number of nodes after split: 235\n",
      "Saved processed JSON to: C:/Users/yigit/Desktop/Enterprises/arayuz-4\\okumalar-prep\\6) Sivil Toplum Örgütlerinde Profesyonel ve Gönüllü Çalışma İlişkileri Tehditler Ve Fırsatlar.json\n",
      "\n",
      "Processing JSON file: 7) eşitsiz demokrasiler.json\n",
      "Number of nodes before split: 430\n",
      "Number of nodes after split: 519\n",
      "Saved processed JSON to: C:/Users/yigit/Desktop/Enterprises/arayuz-4\\okumalar-prep\\7) eşitsiz demokrasiler.json\n",
      "\n",
      "Processing JSON file: 8) genc-oy-strateji-rapor.json\n",
      "Number of nodes before split: 1709\n",
      "Number of nodes after split: 1711\n",
      "Saved processed JSON to: C:/Users/yigit/Desktop/Enterprises/arayuz-4\\okumalar-prep\\8) genc-oy-strateji-rapor.json\n",
      "\n",
      "Processing JSON file: 9) Politik Karar Verme Süreçlerine Etkili ve Anlamlı KATILIM HAKKI ve MEKANİZMALAR.json\n",
      "Number of nodes before split: 1331\n",
      "Number of nodes after split: 1875\n",
      "Saved processed JSON to: C:/Users/yigit/Desktop/Enterprises/arayuz-4\\okumalar-prep\\9) Politik Karar Verme Süreçlerine Etkili ve Anlamlı KATILIM HAKKI ve MEKANİZMALAR.json\n"
     ]
    }
   ],
   "source": [
    "# Now process all JSON files\n",
    "json_files = [f for f in os.listdir(json_dir) if f.lower().endswith('.json')]\n",
    "\n",
    "# Create the sibling \"okumalar-prep\" directory\n",
    "prep_dir = os.path.join(os.path.dirname(json_dir.rstrip(\"/\\\\\")), \"okumalar-prep\")\n",
    "os.makedirs(prep_dir, exist_ok=True)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=30,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "\n",
    "if not json_files:\n",
    "    print(\"No JSON files found in the directory.\")\n",
    "else:\n",
    "    for json_file in sorted(json_files):\n",
    "        json_path = os.path.join(json_dir, json_file)\n",
    "        print(f\"\\nProcessing JSON file: {json_file}\")\n",
    "\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "        # Use enumerate_text_parts_with_position to assign position numbers before removing coordinates\n",
    "        if isinstance(json_data, list):\n",
    "            enumerated_json_data = enumerate_text_parts_with_position(json_data)\n",
    "        else:\n",
    "            enumerated_json_data = json_data  # fallback, should be a list\n",
    "\n",
    "        # Remove specified fields from metadata in every dict in the list\n",
    "        # Also, remove all occurrences of \"- \" (dash and space) that are preceded and followed by letters in the 'text' field\n",
    "        fields_to_remove = [\"coordinates\", \"file_directory\", \"filetype\", \"last_modified\"]\n",
    "        processed_data = []\n",
    "        if isinstance(enumerated_json_data, list):\n",
    "            for item in enumerated_json_data:\n",
    "                if isinstance(item, dict):\n",
    "                    # Remove \"- \" between letters in 'text'\n",
    "                    if \"text\" in item and isinstance(item[\"text\"], str):\n",
    "                        item[\"text\"] = re.sub(r'(?<=[A-Za-zÇĞİÖŞÜçğıöşü])\\- (?=[A-Za-zÇĞİÖŞÜçğıöşü])', '', item[\"text\"])\n",
    "                    # Wrap metadata\n",
    "                    wrapped = wrap_metadata(item)\n",
    "                    # Remove specified fields from metadata\n",
    "                    wrapped[\"metadata\"] = remove_metadata_fields(wrapped.get(\"metadata\", {}), fields_to_remove)\n",
    "                    # Convert languages field from list to string if needed\n",
    "                    wrapped[\"metadata\"] = convert_languages_field_in_metadata(wrapped.get(\"metadata\", {}))\n",
    "                    # Replace nulls in wrapped and metadata\n",
    "                    wrapped = replace_nulls(wrapped)\n",
    "                    processed_data.append(wrapped)\n",
    "                else:\n",
    "                    processed_data.append(replace_nulls(item))\n",
    "        else:\n",
    "            item = enumerated_json_data\n",
    "            if isinstance(item, dict):\n",
    "                if \"text\" in item and isinstance(item[\"text\"], str):\n",
    "                    item[\"text\"] = re.sub(r'(?<=[A-Za-zÇĞİÖŞÜçğıöşü])\\- (?=[A-Za-zÇĞİÖŞÜçğıöşü])', '', item[\"text\"])\n",
    "                wrapped = wrap_metadata(item)\n",
    "                wrapped[\"metadata\"] = remove_metadata_fields(wrapped.get(\"metadata\", {}), fields_to_remove)\n",
    "                wrapped[\"metadata\"] = convert_languages_field_in_metadata(wrapped.get(\"metadata\", {}))\n",
    "                wrapped = replace_nulls(wrapped)\n",
    "                processed_data = [wrapped]\n",
    "            else:\n",
    "                processed_data = [replace_nulls(item)]\n",
    "\n",
    "        # Add \"title\" field to each dict, using the text of the most recent preceding Title-type dict (by position)\n",
    "        # First, build a mapping from position to (type, text)\n",
    "        position_to_title = {}\n",
    "        last_title_text = None\n",
    "        # Sort processed_data by position if available\n",
    "\n",
    "        sorted_data = sorted(processed_data, key=get_position)\n",
    "        for item in sorted_data:\n",
    "            item_type = get_type(item)\n",
    "            item_text = get_text(item)\n",
    "            item_position = get_position(item)\n",
    "            if item_type == \"Title\" and item_text:\n",
    "                last_title_text = item_text\n",
    "            if item_position is not None:\n",
    "                position_to_title[item_position] = last_title_text\n",
    "\n",
    "        # Now, assign the \"title\" field to each item (in metadata)\n",
    "        for item in sorted_data:\n",
    "            item_position = get_position(item)\n",
    "            title_val = position_to_title.get(item_position, None)\n",
    "            # Place \"title\" in metadata\n",
    "            if isinstance(item, dict):\n",
    "                if \"metadata\" not in item or not isinstance(item[\"metadata\"], dict):\n",
    "                    item[\"metadata\"] = {}\n",
    "                # If title_val is None, set to a space\n",
    "                item[\"metadata\"][\"title\"] = title_val if title_val is not None else \" \"\n",
    "\n",
    "        # --- MERGE LOGIC: merge consecutive nodes with same metadata (except position) and total text length < 200 ---\n",
    "        merged_data = []\n",
    "        i = 0\n",
    "        while i < len(sorted_data):\n",
    "            current = sorted_data[i]\n",
    "            if (\n",
    "                i + 1 < len(sorted_data)\n",
    "                and can_merge_nodes(current, sorted_data[i + 1])\n",
    "            ):\n",
    "                # Merge current and next\n",
    "                merged = merge_nodes(current, sorted_data[i + 1])\n",
    "                merged_data.append(merged)\n",
    "                i += 2  # skip next\n",
    "            else:\n",
    "                merged_data.append(current)\n",
    "                i += 1\n",
    "\n",
    "        # Print number of nodes before split\n",
    "        print(f\"Number of nodes before split: {len(merged_data)}\")\n",
    "\n",
    "        # Split text and propagate the \"title\" field\n",
    "        split_json_data = []\n",
    "        for item in merged_data:\n",
    "            if isinstance(item, dict) and \"text\" in item and isinstance(item[\"text\"], str):\n",
    "                splits = text_splitter.split_text(item[\"text\"])\n",
    "                for split_text in splits:\n",
    "                    new_item = dict(item)  # shallow copy\n",
    "                    new_item[\"text\"] = split_text\n",
    "                    # Ensure no nulls in new_item or its metadata\n",
    "                    new_item = replace_nulls(new_item)\n",
    "                    split_json_data.append(new_item)\n",
    "            else:\n",
    "                split_json_data.append(replace_nulls(item))\n",
    "\n",
    "        # Print number of nodes after split\n",
    "        print(f\"Number of nodes after split: {len(split_json_data)}\")\n",
    "\n",
    "        # Save the processed/split JSON to the prep directory\n",
    "        prep_json_path = os.path.join(prep_dir, json_file)\n",
    "        with open(prep_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(split_json_data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Saved processed JSON to: {prep_json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb23814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Path to the prep folder and the target file\n",
    "# prep_dir = os.path.join(os.path.dirname(json_dir.rstrip(\"/\\\\\")), \"okumalar-prep\")\n",
    "# target_filename = \"7) eşitsiz demokrasiler.json\"\n",
    "# target_path = os.path.join(prep_dir, target_filename)\n",
    "\n",
    "# # Load the JSON data\n",
    "# with open(target_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# # Extract the 'text' field from each element\n",
    "# texts = [element['text'] for element in data if 'text' in element]\n",
    "\n",
    "# # Compute statistics\n",
    "# lengths = [len(text) for text in texts]\n",
    "# count = len(lengths)\n",
    "# average = sum(lengths) / count if count > 0 else 0\n",
    "# max_len = max(lengths) if lengths else 0\n",
    "# min_len = min(lengths) if lengths else 0\n",
    "# median_len = sorted(lengths)[len(lengths) // 2] if lengths else 0\n",
    "\n",
    "# print(f\"Summary statistics for 'text' field in '{target_filename}':\")\n",
    "# print(f\"Number of elements: {count}\")\n",
    "# print(f\"Average length: {average:.2f} characters\")\n",
    "# print(f\"Median length: {median_len:.2f} characters\")\n",
    "# print(f\"Max length: {max_len} characters\")\n",
    "# print(f\"Min length: {min_len} characters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0494c0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics for 'text' field across all files in 'C:/Users/yigit/Desktop/Enterprises/arayuz-4\\okumalar-prep':\n",
      "Number of elements: 30020\n",
      "Average length: 123.03 characters\n",
      "Median length: 89.00 characters\n",
      "Max length: 300 characters\n",
      "Min length: 1 characters\n"
     ]
    }
   ],
   "source": [
    "# Path to the prep folder\n",
    "prep_dir = os.path.join(os.path.dirname(json_dir.rstrip(\"/\\\\\")), \"okumalar-prep\")\n",
    "\n",
    "all_texts = []\n",
    "\n",
    "# Iterate over all JSON files in the prep directory and collect all 'text' fields\n",
    "for json_file in glob.glob(os.path.join(prep_dir, \"*.json\")):\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    all_texts.extend([element['text'] for element in data if 'text' in element])\n",
    "\n",
    "# Compute statistics across all files\n",
    "lengths = [len(text) for text in all_texts]\n",
    "count = len(lengths)\n",
    "average = sum(lengths) / count if count > 0 else 0\n",
    "max_len = max(lengths) if lengths else 0\n",
    "min_len = min(lengths) if lengths else 0\n",
    "median_len = sorted(lengths)[len(lengths) // 2] if lengths else 0\n",
    "\n",
    "print(f\"Summary statistics for 'text' field across all files in '{prep_dir}':\")\n",
    "print(f\"Number of elements: {count}\")\n",
    "print(f\"Average length: {average:.2f} characters\")\n",
    "print(f\"Median length: {median_len:.2f} characters\")\n",
    "print(f\"Max length: {max_len} characters\")\n",
    "print(f\"Min length: {min_len} characters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee92bad5",
   "metadata": {},
   "outputs": [
    {
     "ename": "PineconeApiException",
     "evalue": "(400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'Date': 'Fri, 25 Jul 2025 21:09:48 GMT', 'Content-Type': 'application/json', 'Content-Length': '150', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '270', 'x-pinecone-request-id': '5045374394484409737', 'x-envoy-upstream-service-time': '2', 'server': 'envoy'})\nHTTP response body: {\"code\":3,\"message\":\"Metadata value must be a string, number, boolean or list of strings, got '[{\\\"start_index\\\":...' for field 'links'\",\"details\":[]}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPineconeApiException\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m embeddings = OpenAIEmbeddings(model=\u001b[33m\"\u001b[39m\u001b[33mtext-embedding-3-small\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# 3. Upsert all documents into Pinecone vector store\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m vectorstore = \u001b[43mPineconeVectorStore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_documents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_name\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yigit\\Desktop\\Enterprises\\arayuz-4\\.venv\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:848\u001b[39m, in \u001b[36mVectorStore.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, **kwargs)\u001b[39m\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[32m    846\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m] = ids\n\u001b[32m--> \u001b[39m\u001b[32m848\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yigit\\Desktop\\Enterprises\\arayuz-4\\.venv\\Lib\\site-packages\\langchain_pinecone\\vectorstores.py:848\u001b[39m, in \u001b[36mPineconeVectorStore.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, batch_size, text_key, namespace, index_name, upsert_kwargs, pool_threads, embeddings_chunk_size, async_req, id_prefix, **kwargs)\u001b[39m\n\u001b[32m    845\u001b[39m pinecone_index = \u001b[38;5;28mcls\u001b[39m.get_pinecone_index(index_name, pool_threads)\n\u001b[32m    846\u001b[39m pinecone = \u001b[38;5;28mcls\u001b[39m(pinecone_index, embedding, text_key, namespace, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m848\u001b[39m \u001b[43mpinecone\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    850\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_chunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings_chunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    855\u001b[39m \u001b[43m    \u001b[49m\u001b[43masync_req\u001b[49m\u001b[43m=\u001b[49m\u001b[43masync_req\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[43m    \u001b[49m\u001b[43mid_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mid_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupsert_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    858\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pinecone\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yigit\\Desktop\\Enterprises\\arayuz-4\\.venv\\Lib\\site-packages\\langchain_pinecone\\vectorstores.py:335\u001b[39m, in \u001b[36mPineconeVectorStore.add_texts\u001b[39m\u001b[34m(self, texts, metadatas, ids, namespace, batch_size, embedding_chunk_size, async_req, id_prefix, **kwargs)\u001b[39m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m async_req:\n\u001b[32m    325\u001b[39m     \u001b[38;5;66;03m# Runs the pinecone upsert asynchronously.\u001b[39;00m\n\u001b[32m    326\u001b[39m     async_res = [\n\u001b[32m    327\u001b[39m         \u001b[38;5;28mself\u001b[39m.index.upsert(\n\u001b[32m    328\u001b[39m             vectors=batch_vector_tuples,\n\u001b[32m   (...)\u001b[39m\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m batch_vector_tuples \u001b[38;5;129;01min\u001b[39;00m batch_iterate(batch_size, vector_tuples)\n\u001b[32m    334\u001b[39m     ]\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     \u001b[43m[\u001b[49m\u001b[43mres\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mres\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43masync_res\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28mself\u001b[39m.index.upsert(\n\u001b[32m    338\u001b[39m         vectors=vector_tuples,\n\u001b[32m    339\u001b[39m         namespace=namespace,\n\u001b[32m    340\u001b[39m         async_req=async_req,\n\u001b[32m    341\u001b[39m         **kwargs,\n\u001b[32m    342\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yigit\\Desktop\\Enterprises\\arayuz-4\\.venv\\Lib\\site-packages\\langchain_pinecone\\vectorstores.py:335\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m async_req:\n\u001b[32m    325\u001b[39m     \u001b[38;5;66;03m# Runs the pinecone upsert asynchronously.\u001b[39;00m\n\u001b[32m    326\u001b[39m     async_res = [\n\u001b[32m    327\u001b[39m         \u001b[38;5;28mself\u001b[39m.index.upsert(\n\u001b[32m    328\u001b[39m             vectors=batch_vector_tuples,\n\u001b[32m   (...)\u001b[39m\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m batch_vector_tuples \u001b[38;5;129;01min\u001b[39;00m batch_iterate(batch_size, vector_tuples)\n\u001b[32m    334\u001b[39m     ]\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     [\u001b[43mres\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m async_res]\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28mself\u001b[39m.index.upsert(\n\u001b[32m    338\u001b[39m         vectors=vector_tuples,\n\u001b[32m    339\u001b[39m         namespace=namespace,\n\u001b[32m    340\u001b[39m         async_req=async_req,\n\u001b[32m    341\u001b[39m         **kwargs,\n\u001b[32m    342\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\pool.py:774\u001b[39m, in \u001b[36mApplyResult.get\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    772\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\pool.py:125\u001b[39m, in \u001b[36mworker\u001b[39m\u001b[34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[39m\n\u001b[32m    123\u001b[39m job, i, func, args, kwds = task\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     result = (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m wrap_exception \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _helper_reraises_exception:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yigit\\Desktop\\Enterprises\\arayuz-4\\.venv\\Lib\\site-packages\\pinecone\\openapi_support\\api_client.py:182\u001b[39m, in \u001b[36mApiClient.__call_api\u001b[39m\u001b[34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[39m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m PineconeApiException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m     e.body = e.body.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    184\u001b[39m \u001b[38;5;28mself\u001b[39m.last_response = response_data\n\u001b[32m    186\u001b[39m return_data = response_data\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yigit\\Desktop\\Enterprises\\arayuz-4\\.venv\\Lib\\site-packages\\pinecone\\openapi_support\\api_client.py:170\u001b[39m, in \u001b[36mApiClient.__call_api\u001b[39m\u001b[34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[39m\n\u001b[32m    161\u001b[39m url = build_request_url(\n\u001b[32m    162\u001b[39m     config=config,\n\u001b[32m    163\u001b[39m     processed_path_params=path_params_tuple,\n\u001b[32m    164\u001b[39m     resource_path=resource_path,\n\u001b[32m    165\u001b[39m     _host=_host,\n\u001b[32m    166\u001b[39m )\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# perform request and return response\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     response_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprocessed_query_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprocessed_post_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m PineconeApiException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m     e.body = e.body.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yigit\\Desktop\\Enterprises\\arayuz-4\\.venv\\Lib\\site-packages\\pinecone\\openapi_support\\api_client.py:386\u001b[39m, in \u001b[36mApiClient.request\u001b[39m\u001b[34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001b[39m\n\u001b[32m    376\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.rest_client.OPTIONS(\n\u001b[32m    377\u001b[39m         url,\n\u001b[32m    378\u001b[39m         query_params=query_params,\n\u001b[32m   (...)\u001b[39m\u001b[32m    383\u001b[39m         body=body,\n\u001b[32m    384\u001b[39m     )\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m method == \u001b[33m\"\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrest_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPOST\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m method == \u001b[33m\"\u001b[39m\u001b[33mPUT\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    396\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.rest_client.PUT(\n\u001b[32m    397\u001b[39m         url,\n\u001b[32m    398\u001b[39m         query_params=query_params,\n\u001b[32m   (...)\u001b[39m\u001b[32m    403\u001b[39m         body=body,\n\u001b[32m    404\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yigit\\Desktop\\Enterprises\\arayuz-4\\.venv\\Lib\\site-packages\\pinecone\\openapi_support\\rest_utils.py:146\u001b[39m, in \u001b[36mRestClientInterface.POST\u001b[39m\u001b[34m(self, url, headers, query_params, post_params, body, _preload_content, _request_timeout)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mPOST\u001b[39m(\n\u001b[32m    137\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    138\u001b[39m     url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    144\u001b[39m     _request_timeout=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    145\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yigit\\Desktop\\Enterprises\\arayuz-4\\.venv\\Lib\\site-packages\\pinecone\\openapi_support\\rest_urllib3.py:267\u001b[39m, in \u001b[36mUrllib3RestClient.request\u001b[39m\u001b[34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001b[39m\n\u001b[32m    264\u001b[39m     \u001b[38;5;66;03m# log response body\u001b[39;00m\n\u001b[32m    265\u001b[39m     logger.debug(\u001b[33m\"\u001b[39m\u001b[33mresponse body: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, r.data)\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mraise_exceptions_or_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yigit\\Desktop\\Enterprises\\arayuz-4\\.venv\\Lib\\site-packages\\pinecone\\openapi_support\\rest_utils.py:49\u001b[39m, in \u001b[36mraise_exceptions_or_return\u001b[39m\u001b[34m(r)\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m500\u001b[39m <= r.status <= \u001b[32m599\u001b[39m:\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ServiceException(http_resp=r)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PineconeApiException(http_resp=r)\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[31mPineconeApiException\u001b[39m: (400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'Date': 'Fri, 25 Jul 2025 21:09:48 GMT', 'Content-Type': 'application/json', 'Content-Length': '150', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '270', 'x-pinecone-request-id': '5045374394484409737', 'x-envoy-upstream-service-time': '2', 'server': 'envoy'})\nHTTP response body: {\"code\":3,\"message\":\"Metadata value must be a string, number, boolean or list of strings, got '[{\\\"start_index\\\":...' for field 'links'\",\"details\":[]}\n"
     ]
    }
   ],
   "source": [
    "# 1. Iterate over all JSON files in prep_dir\n",
    "pdf_dir = \"C:/Users/yigit/Desktop/Enterprises/arayuz-4/okumalar-pdf/\"\n",
    "json_dir = os.path.join(os.path.dirname(pdf_dir.rstrip(\"/\\\\\")), \"okumalar-json\")\n",
    "prep_dir = os.path.join(os.path.dirname(json_dir.rstrip(\"/\\\\\")), \"okumalar-prep\")\n",
    "\n",
    "all_documents = []\n",
    "for json_path in glob.glob(os.path.join(prep_dir, \"*.json\")):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        split_json_data = json.load(f)\n",
    "    # Convert split_json_data to Document objects\n",
    "    documents = [\n",
    "        Document(\n",
    "            page_content=element['text'],\n",
    "            metadata=element['metadata'] | {'element_id': element['element_id']}\n",
    "        )\n",
    "        for element in split_json_data\n",
    "    ]\n",
    "    all_documents.extend(documents)\n",
    "\n",
    "# 2. Create embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# 3. Upsert all documents into Pinecone vector store\n",
    "vectorstore = PineconeVectorStore.from_documents(\n",
    "    documents=all_documents,\n",
    "    embedding=embeddings,\n",
    "    index_name=index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b233dd",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af22049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have the correct index_name and embedding model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectorstore = PineconeVectorStore(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"filename\",\n",
    "        description=\"The name of the PDF document the text comes from\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"languages\",\n",
    "        description=\"The list of language codes used in the document (e.g. 'tur', 'eng')\",\n",
    "        type=\"list[string]\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"page_number\",\n",
    "        description=\"The page number within the PDF document\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"type\",\n",
    "        description=\"The structural type of the text chunk (e.g. Title, Paragraph, List, Quote)\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"title\",\n",
    "        description=\"The section title of the chunk\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "document_content_description = \"Sociological and political researches and analyses\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d933aac1",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562fb1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# retriever = SelfQueryRetriever.from_llm(\n",
    "#     llm=llm,\n",
    "#     vectorstore=vectorstore,\n",
    "#     document_contents=\"text\",\n",
    "#     document_content_description=document_content_description,\n",
    "#     metadata_field_info=metadata_field_info,\n",
    "#     search_kwargs={\"k\": 5}\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88f19bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Simple prompt ---\n",
    "template = \"\"\"Aşağıda bazı metin parçaları verilmiştir. Bu bilgilere dayanarak son kullanıcı sorusuna tarafsız ve güvenli bir şekilde yanıt ver:\n",
    "\n",
    "Metin parçaları:\n",
    "{context}\n",
    "\n",
    "Soru: {question}\n",
    "Yanıt:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c6d57b",
   "metadata": {},
   "source": [
    "#### GPT 3.5 Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd02bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- QA Chain ---\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 4}),\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# --- User question ---\n",
    "query = \"Türkiye büyük millet meclisi ile İspanya parlamentosu arasındaki en önemli farklar nelerdir?\"\n",
    "\n",
    "# --- Get Answer ---\n",
    "result = qa_chain.run(query)\n",
    "print(\"\\nYanıt:\\n\", result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2416b511",
   "metadata": {},
   "source": [
    "#### GPT 3.5 Turbo (Newer Release)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e08f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- QA Chain ---\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0),\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 4}),\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# --- User question ---\n",
    "query = \"Türkiye büyük millet meclisi ile İspanya parlamentosu arasındaki en önemli farklar nelerdir?\"\n",
    "\n",
    "# --- Get Answer ---\n",
    "result = qa_chain.run(query)\n",
    "print(\"\\nYanıt:\\n\", result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a255b9",
   "metadata": {},
   "source": [
    "#### GPT 4.1 Mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d28f396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- QA Chain ---\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model=\"gpt-4.1-mini-2025-04-14\", temperature=0),\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 4}),\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# --- User question ---\n",
    "query = \"Türkiye büyük millet meclisi ile İspanya parlamentosu arasındaki en önemli farklar nelerdir?\"\n",
    "\n",
    "# --- Get Answer ---\n",
    "result = qa_chain.run(query)\n",
    "print(\"\\nYanıt:\\n\", result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbabdf52",
   "metadata": {},
   "source": [
    "#### GPT 4-o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b121c08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- QA Chain ---\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model=\"gpt-4o-2024-08-06\", temperature=0),\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 4}),\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# --- User question ---\n",
    "query = \"Türkiye büyük millet meclisi ile İspanya parlamentosu arasındaki en önemli farklar nelerdir?\"\n",
    "\n",
    "# --- Get Answer ---\n",
    "result = qa_chain.run(query)\n",
    "print(\"\\nYanıt:\\n\", result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f353cbe3",
   "metadata": {},
   "source": [
    "### Google Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f173234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from google.generativeai import GenerativeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ff5e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_api_key = os.getenv('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04924498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Gemini API\n",
    "genai.configure(api_key=google_api_key)\n",
    "\n",
    "class GeminiQAChain:\n",
    "    def __init__(self, model_name=\"gemini-2.5-flash\", vectorstore=None, k=4, prompt_template=None):\n",
    "        \"\"\"\n",
    "        Initialize Gemini QA Chain\n",
    "        \n",
    "        Args:\n",
    "            model_name: \"gemini-2.5-pro\" or \"gemini-2.5-flash\"\n",
    "            vectorstore: Your vector store (same as before)\n",
    "            k: Number of documents to retrieve\n",
    "            prompt_template: Custom prompt template\n",
    "        \"\"\"\n",
    "        self.model = GenerativeModel(model_name)\n",
    "        self.vectorstore = vectorstore\n",
    "        self.k = k\n",
    "        self.prompt_template = prompt_template or self._default_prompt_template()\n",
    "    \n",
    "    def _default_prompt_template(self):\n",
    "        return \"\"\"\n",
    "        Context: {context}\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Please provide a comprehensive answer based on the context provided above. If the context doesn't contain enough information to answer the question, please state that clearly.\n",
    "        \n",
    "        Answer:\n",
    "        \"\"\"\n",
    "    \n",
    "    def run(self, query):\n",
    "        \"\"\"\n",
    "        Run the QA chain with the given query\n",
    "        \"\"\"\n",
    "        # Retrieve relevant documents\n",
    "        if self.vectorstore:\n",
    "            retriever = self.vectorstore.as_retriever(search_kwargs={\"k\": self.k})\n",
    "            docs = retriever.get_relevant_documents(query)\n",
    "            context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "        else:\n",
    "            context = \"No context provided\"\n",
    "        \n",
    "        # Format the prompt\n",
    "        formatted_prompt = self.prompt_template.format(\n",
    "            context=context,\n",
    "            question=query\n",
    "        )\n",
    "        \n",
    "        # Generate response using Gemini\n",
    "        try:\n",
    "            response = self.model.generate_content(\n",
    "                formatted_prompt,\n",
    "                generation_config={\n",
    "                    \"temperature\": 0,\n",
    "                    \"top_p\": 1,\n",
    "                    \"top_k\": 1\n",
    "                }\n",
    "            )\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3173be",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# 2. Instantiate vectorstore without upserting data\n",
    "vectorstore = PineconeVectorStore(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413f6320",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt = \"\"\"\n",
    "Bağlam: {context}\n",
    "\n",
    "Soru: {question}\n",
    "\n",
    "Lütfen yukarıdaki bağlam bilgilerine dayanarak kapsamlı bir yanıt verin. Türkçe yanıtlayın.\n",
    "\n",
    "Yanıt:\n",
    "\"\"\"\n",
    "\n",
    "# --- User question ---\n",
    "query = \"Türkiye büyük millet meclisi ile İspanya parlamentosu arasındaki en önemli farklar nelerdir?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aa27ca",
   "metadata": {},
   "source": [
    "#### Gemini 2.5 Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea77e488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Using Gemini 2.5 Pro (more capable, slower)\n",
    "qa_chain_pro = GeminiQAChain(\n",
    "    model_name=\"gemini-2.5-pro\",\n",
    "    vectorstore=vectorstore,\n",
    "    k=4,\n",
    "    prompt_template=custom_prompt\n",
    ")\n",
    "\n",
    "result_pro = qa_chain_pro.run(query)\n",
    "print(\"\\nGemini 2.5 Pro Yanıtı:\\n\", result_pro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19665ed4",
   "metadata": {},
   "source": [
    "#### Gemini 2.5 Flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfc7c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Using Gemini 2.5 Flash (faster, good performance)\n",
    "qa_chain_flash = GeminiQAChain(\n",
    "    model_name=\"gemini-2.5-flash\",\n",
    "    vectorstore=vectorstore,  # Your existing vectorstore\n",
    "    k=4,\n",
    "    prompt_template=custom_prompt\n",
    ")\n",
    "\n",
    "result_flash = qa_chain_flash.run(query)\n",
    "print(\"\\nGemini 2.5 Flash Yanıtı:\\n\", result_flash)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07e63b9",
   "metadata": {},
   "source": [
    "#### Gemini 2.5 Flash-Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fbafe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Using Gemini 2.5 Flash (faster, good performance)\n",
    "qa_chain_flash = GeminiQAChain(\n",
    "    model_name=\"gemini-2.5-flash-lite\",\n",
    "    vectorstore=vectorstore,  # Your existing vectorstore\n",
    "    k=4,\n",
    "    prompt_template=custom_prompt\n",
    ")\n",
    "\n",
    "result_flash = qa_chain_flash.run(query)\n",
    "print(\"\\nGemini 2.5 Flash-Lite Yanıtı:\\n\", result_flash)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508e9859",
   "metadata": {},
   "source": [
    "### DeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089227a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae41dd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure DeepSeek API (uses OpenAI-compatible interface)\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=deepseek_api_key,\n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a52b562",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSeekQAChain:\n",
    "    def __init__(self, model_name=\"deepseek-chat\", vectorstore=None, k=4, prompt_template=None, temperature=0):\n",
    "        \"\"\"\n",
    "        Initialize DeepSeek QA Chain\n",
    "        \n",
    "        Args:\n",
    "            model_name: \"deepseek-chat\" (main model)\n",
    "            vectorstore: Your vector store\n",
    "            k: Number of documents to retrieve\n",
    "            prompt_template: Custom prompt template\n",
    "            temperature: 0 for deterministic, higher for creative\n",
    "        \"\"\"\n",
    "        self.client = client\n",
    "        self.model_name = model_name\n",
    "        self.vectorstore = vectorstore\n",
    "        self.k = k\n",
    "        self.temperature = temperature\n",
    "        self.prompt_template = prompt_template or self._default_prompt_template()\n",
    "    \n",
    "    def _default_prompt_template(self):\n",
    "        return \"\"\"\n",
    "        Context: {context}\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Please provide a comprehensive answer based on the context provided above. If the context doesn't contain enough information to answer the question, please state that clearly.\n",
    "        \n",
    "        Answer:\n",
    "        \"\"\"\n",
    "    \n",
    "    def run(self, query):\n",
    "        \"\"\"\n",
    "        Run the QA chain with the given query\n",
    "        \"\"\"\n",
    "        # Retrieve relevant documents\n",
    "        if self.vectorstore:\n",
    "            retriever = self.vectorstore.as_retriever(search_kwargs={\"k\": self.k})\n",
    "            docs = retriever.get_relevant_documents(query)\n",
    "            context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "        else:\n",
    "            context = \"No context provided\"\n",
    "        \n",
    "        # Format the prompt\n",
    "        formatted_prompt = self.prompt_template.format(\n",
    "            context=context,\n",
    "            question=query\n",
    "        )\n",
    "        \n",
    "        # Generate response using DeepSeek\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": formatted_prompt}\n",
    "                ],\n",
    "                temperature=self.temperature,\n",
    "                max_tokens=2048,\n",
    "                top_p=1,\n",
    "                frequency_penalty=0,\n",
    "                presence_penalty=0,\n",
    "                stream=False\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddf4f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Custom prompt for Turkish content ---\n",
    "custom_prompt = \"\"\"\n",
    "Bağlam: {context}\n",
    "\n",
    "Soru: {question}\n",
    "\n",
    "Lütfen yukarıdaki bağlam bilgilerine dayanarak detaylı ve kapsamlı bir yanıt ver. Yanıtınızı Türkçe olarak yaz ve mümkün olduğunca objektif bir şekilde karşılaştırma yap.\n",
    "\n",
    "Yanıt:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fa2d92",
   "metadata": {},
   "source": [
    "#### DeepSeek Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267bfa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Chat (general purpose, excellent reasoning)\n",
    "qa_chain_chat = DeepSeekQAChain(\n",
    "    model_name=\"deepseek-chat\",\n",
    "    vectorstore=vectorstore,  # Your existing vectorstore\n",
    "    k=4,\n",
    "    temperature=0,\n",
    "    prompt_template=custom_prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7774dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- User question ---\n",
    "query = \"Türkiye büyük millet meclisi ile İspanya parlamentosu arasındaki en önemli farklar nelerdir?\"\n",
    "\n",
    "# --- Get Answer ---\n",
    "# Using DeepSeek Chat\n",
    "result_chat = qa_chain_chat.run(query)\n",
    "print(\"\\nDeepSeek Chat Yanıtı:\\n\", result_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a92ac0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
