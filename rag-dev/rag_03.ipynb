{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5d01211",
   "metadata": {},
   "source": [
    "# Indexing and Other Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e33a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install uv\n",
    "# uv init\n",
    "# uv add unstructured\n",
    "# uv add \"unstructured[pdf]\"\n",
    "# uv pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e607a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yigit\\Desktop\\Enterprises\\arayuz-4\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.staging.base import elements_to_json\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "import requests\n",
    "\n",
    "# Standard imports\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.schema import Document\n",
    "from langchain.chains.query_constructor.schema import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "import lark\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ba31f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "langchain_tracing_v2 = os.getenv('LANGCHAIN_TRACING_V2')\n",
    "langchain_endpoint = os.getenv('LANGCHAIN_ENDPOINT')\n",
    "langchain_api_key = os.getenv('LANGCHAIN_API_KEY')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "index_name = os.getenv('PINECONE_INDEX_NAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c93d98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enumerate_text_parts_with_position(text_elements, exclude_types=None, column_threshold=50):\n",
    "    \"\"\"\n",
    "    Enumerate consecutive parts of a page in reading order, assigning a 'position' field to each.\n",
    "    Footer always gets the last position in a page.\n",
    "    Returns a list of dicts, each with 'position', 'text', and other original fields.\n",
    "    \n",
    "    Args:\n",
    "        text_elements (list): List of text elements with coordinates and type\n",
    "        exclude_types (list): List of element types to exclude (except 'footer', which is handled specially)\n",
    "        column_threshold (int): Pixel threshold to determine column boundaries\n",
    "    \n",
    "    Returns:\n",
    "        list: List of dicts, each with 'position', 'text', and other original fields\n",
    "    \"\"\"\n",
    "    # Always process footers separately, regardless of exclude_types\n",
    "    if exclude_types is None:\n",
    "        exclude_types = []\n",
    "    exclude_types_no_footer = [t for t in exclude_types if t.lower() != 'footer']\n",
    "\n",
    "    # Split elements into footers and non-footers\n",
    "    footer_elements = [\n",
    "        elem for elem in text_elements\n",
    "        if elem.get('type', '').lower() == 'footer'\n",
    "    ]\n",
    "    non_footer_elements = [\n",
    "        elem for elem in text_elements\n",
    "        if elem.get('type', '').lower() not in [t.lower() for t in exclude_types_no_footer + ['footer']]\n",
    "    ]\n",
    "\n",
    "    # If there are no non-footer elements and no footers, return empty\n",
    "    if not non_footer_elements and not footer_elements:\n",
    "        return []\n",
    "\n",
    "    # Group non-footer elements by approximate Y position (rows)\n",
    "    rows = {}\n",
    "    for element in non_footer_elements:\n",
    "        coords = element.get('coordinates', [[0, 0]])\n",
    "        if coords:\n",
    "            x, y = coords[0]  # Upper-left corner\n",
    "            row_key = round(y / 20) * 20  # Group by 20-pixel rows\n",
    "            if row_key not in rows:\n",
    "                rows[row_key] = []\n",
    "            rows[row_key].append(element)\n",
    "\n",
    "    # Sort rows by Y position (top to bottom)\n",
    "    sorted_rows = sorted(rows.items())\n",
    "\n",
    "    # Flatten elements in reading order\n",
    "    ordered_elements = []\n",
    "    for row_y, row_elements in sorted_rows:\n",
    "        row_elements.sort(key=lambda elem: elem.get('coordinates', [[0, 0]])[0][0])\n",
    "        ordered_elements.extend(row_elements)\n",
    "\n",
    "    # Enumerate and assign position to non-footer elements\n",
    "    enumerated_parts = []\n",
    "    position = 0\n",
    "    for element in ordered_elements:\n",
    "        text = element.get('text', '').strip()\n",
    "        if text:\n",
    "            text = re.sub(r' +', ' ', text)\n",
    "            part = dict(element)\n",
    "            part['text'] = text\n",
    "            part['position'] = position\n",
    "            enumerated_parts.append(part)\n",
    "            position += 1\n",
    "\n",
    "    # Now process footers, assign them the last position(s)\n",
    "    for i, element in enumerate(footer_elements):\n",
    "        text = element.get('text', '').strip()\n",
    "        if text:\n",
    "            text = re.sub(r' +', ' ', text)\n",
    "            part = dict(element)\n",
    "            part['text'] = text\n",
    "            # If multiple footers, assign consecutive positions after non-footers\n",
    "            part['position'] = position + i\n",
    "            enumerated_parts.append(part)\n",
    "\n",
    "    return enumerated_parts\n",
    "\n",
    "\n",
    "def wrap_metadata(item):\n",
    "    \"\"\"Wrap all fields except 'element_id' and 'text' into a 'metadata' dict. \n",
    "    The values of metadata fields are the original values (including dicts).\"\"\"\n",
    "    if not isinstance(item, dict):\n",
    "        return item\n",
    "    new_item = {}\n",
    "    # Always keep 'element_id' and 'text' at top level if present\n",
    "    if \"element_id\" in item:\n",
    "        new_item[\"element_id\"] = item[\"element_id\"]\n",
    "    if \"text\" in item:\n",
    "        new_item[\"text\"] = item[\"text\"]\n",
    "    # Everything else goes into metadata\n",
    "    metadata = {}\n",
    "    for k, v in item.items():\n",
    "        if k not in (\"element_id\", \"text\", \"metadata\"):\n",
    "            metadata[k] = v\n",
    "        elif k == \"metadata\" and isinstance(v, dict):\n",
    "            # Merge existing metadata fields\n",
    "            for mk, mv in v.items():\n",
    "                metadata[mk] = mv\n",
    "    new_item[\"metadata\"] = metadata\n",
    "    return new_item\n",
    "\n",
    "def clean_metadata_for_pinecone(metadata):\n",
    "    \"\"\"\n",
    "    Clean metadata to ensure compatibility with Pinecone.\n",
    "    Pinecone only supports: string, number, boolean, or list of strings.\n",
    "    \"\"\"\n",
    "    if not isinstance(metadata, dict):\n",
    "        return metadata\n",
    "    \n",
    "    cleaned = {}\n",
    "    for key, value in metadata.items():\n",
    "        if value is None:\n",
    "            cleaned[key] = \" \"  # Replace None with space\n",
    "        elif isinstance(value, (str, int, float, bool)):\n",
    "            cleaned[key] = value  # Keep simple types as-is\n",
    "        elif isinstance(value, list):\n",
    "            # Handle lists - convert to list of strings or filter out complex objects\n",
    "            if all(isinstance(item, (str, int, float, bool)) for item in value):\n",
    "                # List of simple types - convert all to strings\n",
    "                cleaned[key] = [str(item) for item in value]\n",
    "            elif len(value) == 1 and isinstance(value[0], (str, int, float, bool)):\n",
    "                # Single item list - convert to string\n",
    "                cleaned[key] = str(value[0])\n",
    "            else:\n",
    "                # Complex list or mixed types - skip this field or convert to string representation\n",
    "                print(f\"Warning: Skipping complex list field '{key}': {value}\")\n",
    "                # Option 1: Skip the field entirely\n",
    "                continue\n",
    "                # Option 2: Convert to string representation (uncomment line below)\n",
    "                # cleaned[key] = str(value)[:500]  # Limit length\n",
    "        elif isinstance(value, dict):\n",
    "            # Complex object - skip or convert to string\n",
    "            print(f\"Warning: Skipping complex object field '{key}': {value}\")\n",
    "            # Option 1: Skip the field entirely\n",
    "            continue\n",
    "            # Option 2: Convert to string representation (uncomment line below)\n",
    "            # cleaned[key] = str(value)[:500]  # Limit length\n",
    "        else:\n",
    "            # Any other type - convert to string\n",
    "            cleaned[key] = str(value)[:500]  # Limit length to avoid huge strings\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def enhanced_wrap_metadata(item):\n",
    "    \"\"\"\n",
    "    Enhanced version of wrap_metadata that includes cleaning for Pinecone compatibility.\n",
    "    \"\"\"\n",
    "    if not isinstance(item, dict):\n",
    "        return item\n",
    "    \n",
    "    new_item = {}\n",
    "    # Always keep 'element_id' and 'text' at top level if present\n",
    "    if \"element_id\" in item:\n",
    "        new_item[\"element_id\"] = item[\"element_id\"]\n",
    "    if \"text\" in item:\n",
    "        new_item[\"text\"] = item[\"text\"]\n",
    "    \n",
    "    # Everything else goes into metadata\n",
    "    metadata = {}\n",
    "    for k, v in item.items():\n",
    "        if k not in (\"element_id\", \"text\", \"metadata\"):\n",
    "            metadata[k] = v\n",
    "        elif k == \"metadata\" and isinstance(v, dict):\n",
    "            # Merge existing metadata fields\n",
    "            for mk, mv in v.items():\n",
    "                metadata[mk] = mv\n",
    "    \n",
    "    # Clean metadata for Pinecone compatibility\n",
    "    metadata = remove_problematic_metadata_fields(metadata)\n",
    "    metadata = clean_metadata_for_pinecone(metadata)\n",
    "    \n",
    "    new_item[\"metadata\"] = metadata\n",
    "    return new_item\n",
    "\n",
    "def remove_problematic_metadata_fields(metadata, additional_fields_to_remove=None):\n",
    "    \"\"\"\n",
    "    Remove fields that are known to cause issues with Pinecone.\n",
    "    \"\"\"\n",
    "    if not isinstance(metadata, dict):\n",
    "        return metadata\n",
    "    \n",
    "    # Default problematic fields\n",
    "    problematic_fields = [\n",
    "        \"coordinates\", \n",
    "        \"file_directory\", \n",
    "        \"filetype\", \n",
    "        \"last_modified\",\n",
    "        \"links\",  # This is the one causing your error\n",
    "        \"link_urls\",\n",
    "        \"link_texts\", \n",
    "        \"emphasized_text_contents\",\n",
    "        \"emphasized_text_tags\",\n",
    "        \"detection_class_prob\",\n",
    "        \"parent_id\",\n",
    "        \"orig_elements\"\n",
    "    ]\n",
    "    \n",
    "    # Add any additional fields to remove\n",
    "    if additional_fields_to_remove:\n",
    "        problematic_fields.extend(additional_fields_to_remove)\n",
    "    \n",
    "    cleaned = {}\n",
    "    for key, value in metadata.items():\n",
    "        if key not in problematic_fields:\n",
    "            cleaned[key] = value\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def debug_metadata_types(data_sample, max_items=5):\n",
    "    \"\"\"\n",
    "    Debug function to inspect metadata types in your data.\n",
    "    \"\"\"\n",
    "    print(\"üîç Debugging metadata types...\")\n",
    "    \n",
    "    if not isinstance(data_sample, list):\n",
    "        data_sample = [data_sample]\n",
    "    \n",
    "    for i, item in enumerate(data_sample[:max_items]):\n",
    "        print(f\"\\n--- Item {i+1} ---\")\n",
    "        if isinstance(item, dict) and \"metadata\" in item:\n",
    "            metadata = item[\"metadata\"]\n",
    "            print(f\"Metadata keys and types:\")\n",
    "            for key, value in metadata.items():\n",
    "                value_type = type(value).__name__\n",
    "                if isinstance(value, list) and value:\n",
    "                    inner_types = set(type(x).__name__ for x in value[:3])  # Check first 3 items\n",
    "                    print(f\"  {key}: {value_type} (contains: {inner_types}) - {str(value)[:100]}...\")\n",
    "                else:\n",
    "                    print(f\"  {key}: {value_type} - {str(value)[:100]}...\")\n",
    "        else:\n",
    "            print(\"No metadata found or item is not a dict\")\n",
    "\n",
    "def remove_metadata_fields(metadata, fields_to_remove):\n",
    "    \"\"\"Remove specified fields from a metadata dict.\"\"\"\n",
    "    if not isinstance(metadata, dict):\n",
    "        return metadata\n",
    "    for field in fields_to_remove:\n",
    "        metadata.pop(field, None)\n",
    "    return metadata\n",
    "\n",
    "def convert_languages_field_in_metadata(metadata):\n",
    "    \"\"\"If 'languages' in metadata is a list of length 1, convert it to a string.\"\"\"\n",
    "    if isinstance(metadata, dict) and \"languages\" in metadata:\n",
    "        if isinstance(metadata[\"languages\"], list) and len(metadata[\"languages\"]) == 1:\n",
    "            metadata[\"languages\"] = metadata[\"languages\"][0]\n",
    "    return metadata\n",
    "\n",
    "def get_position(x):\n",
    "    if isinstance(x, dict):\n",
    "        # Try top-level, then metadata\n",
    "        if \"position\" in x:\n",
    "            return x[\"position\"]\n",
    "        elif \"metadata\" in x and isinstance(x[\"metadata\"], dict) and \"position\" in x[\"metadata\"]:\n",
    "            return x[\"metadata\"][\"position\"]\n",
    "    return -1\n",
    "\n",
    "def get_type(x):\n",
    "    if isinstance(x, dict):\n",
    "        if \"type\" in x:\n",
    "            return x[\"type\"]\n",
    "        elif \"metadata\" in x and isinstance(x[\"metadata\"], dict) and \"type\" in x[\"metadata\"]:\n",
    "            return x[\"metadata\"][\"type\"]\n",
    "    return None\n",
    "\n",
    "def get_text(x):\n",
    "    if isinstance(x, dict):\n",
    "        return x.get(\"text\", None)\n",
    "    return None\n",
    "\n",
    "def replace_nulls(obj):\n",
    "    \"\"\"Recursively replace None values in dicts/lists with a space character.\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: replace_nulls(v) if v is not None else \" \" for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [replace_nulls(v) for v in obj]\n",
    "    else:\n",
    "        return obj if obj is not None else \" \"\n",
    "    \n",
    "def metadata_without_position(meta):\n",
    "    \"\"\"Return a copy of metadata dict without the 'position' field.\"\"\"\n",
    "    if not isinstance(meta, dict):\n",
    "        return meta\n",
    "    return {k: v for k, v in meta.items() if k != \"position\"}\n",
    "\n",
    "def can_merge_nodes(node1, node2):\n",
    "    \"\"\"Return True if node1 and node2 can be merged according to the rules.\"\"\"\n",
    "    if not (isinstance(node1, dict) and isinstance(node2, dict)):\n",
    "        return False\n",
    "    meta1 = node1.get(\"metadata\", {})\n",
    "    meta2 = node2.get(\"metadata\", {})\n",
    "    # Compare all metadata fields except 'position'\n",
    "    if metadata_without_position(meta1) != metadata_without_position(meta2):\n",
    "        return False\n",
    "    # Both must have 'text' fields that are strings\n",
    "    text1 = node1.get(\"text\", \"\")\n",
    "    text2 = node2.get(\"text\", \"\")\n",
    "    if not (isinstance(text1, str) and isinstance(text2, str)):\n",
    "        return False\n",
    "    # Total length of text fields must be less than 200\n",
    "    if len(text1) + len(text2) < 200:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def merge_nodes(node1, node2):\n",
    "    \"\"\"Merge node2 into node1, concatenating text and updating position to the lower of the two.\"\"\"\n",
    "    merged = dict(node1)\n",
    "    merged[\"text\"] = node1.get(\"text\", \"\") + \" \" + node2.get(\"text\", \"\")\n",
    "    # Merge metadata, keep the lower position\n",
    "    meta1 = node1.get(\"metadata\", {})\n",
    "    meta2 = node2.get(\"metadata\", {})\n",
    "    merged_meta = dict(meta1)\n",
    "    pos1 = meta1.get(\"position\", node1.get(\"position\", None))\n",
    "    pos2 = meta2.get(\"position\", node2.get(\"position\", None))\n",
    "    # Use the lower position if both exist\n",
    "    if pos1 is not None and pos2 is not None:\n",
    "        merged_meta[\"position\"] = min(pos1, pos2)\n",
    "    elif pos1 is not None:\n",
    "        merged_meta[\"position\"] = pos1\n",
    "    elif pos2 is not None:\n",
    "        merged_meta[\"position\"] = pos2\n",
    "    merged[\"metadata\"] = merged_meta\n",
    "    return merged\n",
    "\n",
    "def process_json_data_enhanced(enumerated_json_data):\n",
    "    \"\"\"\n",
    "    Enhanced version of _process_json_data with better metadata cleaning.\n",
    "    \"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    if isinstance(enumerated_json_data, list):\n",
    "        for item in enumerated_json_data:\n",
    "            if isinstance(item, dict):\n",
    "                # Clean text\n",
    "                if \"text\" in item and isinstance(item[\"text\"], str):\n",
    "                    item[\"text\"] = re.sub(r'(?<=[A-Za-z√áƒûƒ∞√ñ≈û√ú√ßƒüƒ±√∂≈ü√º])\\- (?=[A-Za-z√áƒûƒ∞√ñ≈û√ú√ßƒüƒ±√∂≈ü√º])', '', item[\"text\"])\n",
    "                \n",
    "                # Enhanced metadata wrapping with cleaning\n",
    "                wrapped = enhanced_wrap_metadata(item)\n",
    "                wrapped = replace_nulls(wrapped)\n",
    "                processed_data.append(wrapped)\n",
    "            else:\n",
    "                processed_data.append(replace_nulls(item))\n",
    "    \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39615c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All PDFs have been processed to JSON.\n"
     ]
    }
   ],
   "source": [
    "pdf_dir = \"C:/Users/yigit/Desktop/Enterprises/arayuz-4/okumalar-pdf/\"\n",
    "json_dir = os.path.join(os.path.dirname(pdf_dir.rstrip(\"/\\\\\")), \"okumalar-json\")\n",
    "\n",
    "os.makedirs(json_dir, exist_ok=True)\n",
    "\n",
    "# Get all PDF and JSON base filenames (without extension)\n",
    "pdf_files = [f for f in os.listdir(pdf_dir) if f.lower().endswith(\".pdf\")]\n",
    "json_files = [f for f in os.listdir(json_dir) if f.lower().endswith('.json')]\n",
    "\n",
    "pdf_basenames = set(os.path.splitext(f)[0] for f in pdf_files)\n",
    "json_basenames = set(os.path.splitext(f)[0] for f in json_files)\n",
    "\n",
    "# Find PDFs that do not have a corresponding JSON\n",
    "unprocessed_basenames = pdf_basenames - json_basenames\n",
    "\n",
    "if not unprocessed_basenames:\n",
    "    print(\"All PDFs have been processed to JSON.\")\n",
    "else:\n",
    "    print(f\"Processing {len(unprocessed_basenames)} unprocessed PDFs...\")\n",
    "    for base_file_name in sorted(unprocessed_basenames):\n",
    "        pdf_path = os.path.join(pdf_dir, f\"{base_file_name}.pdf\")\n",
    "        json_path = os.path.join(json_dir, f\"{base_file_name}.json\")\n",
    "        print(f\"Processing: {pdf_path} -> {json_path}\")\n",
    "        elements = partition_pdf(\n",
    "            filename=pdf_path,\n",
    "            languages=[\"tur\"],\n",
    "            strategy=\"fast\",\n",
    "            infer_table_structure=True,\n",
    "        )\n",
    "        elements_to_json(elements=elements, filename=json_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0225b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced JSON preprocessing with better metadata cleaning\n",
    "json_files = [f for f in os.listdir(json_dir) if f.lower().endswith('.json')]\n",
    "\n",
    "# Create the sibling \"okumalar-prep\" directory\n",
    "prep_dir = os.path.join(os.path.dirname(json_dir.rstrip(\"/\\\\\")), \"okumalar-prep\")\n",
    "os.makedirs(prep_dir, exist_ok=True)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=30,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "if not json_files:\n",
    "    print(\"No JSON files found in the directory.\")\n",
    "else:\n",
    "    for json_file in sorted(json_files):\n",
    "        json_path = os.path.join(json_dir, json_file)\n",
    "        print(f\"\\nProcessing JSON file: {json_file}\")\n",
    "\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "        # Use enumerate_text_parts_with_position to assign position numbers\n",
    "        if isinstance(json_data, list):\n",
    "            enumerated_json_data = enumerate_text_parts_with_position(json_data)\n",
    "        else:\n",
    "            enumerated_json_data = json_data\n",
    "\n",
    "        # ENHANCED PROCESSING - Use the new cleaning functions\n",
    "        processed_data = []\n",
    "        if isinstance(enumerated_json_data, list):\n",
    "            for item in enumerated_json_data:\n",
    "                if isinstance(item, dict):\n",
    "                    # Remove \"- \" between letters in 'text'\n",
    "                    if \"text\" in item and isinstance(item[\"text\"], str):\n",
    "                        item[\"text\"] = re.sub(r'(?<=[A-Za-z√áƒûƒ∞√ñ≈û√ú√ßƒüƒ±√∂≈ü√º])\\\\- (?=[A-Za-z√áƒûƒ∞√ñ≈û√ú√ßƒüƒ±√∂≈ü√º])', '', item[\"text\"])\n",
    "                    \n",
    "                    # ENHANCED: Use the new enhanced_wrap_metadata function\n",
    "                    wrapped = enhanced_wrap_metadata(item)\n",
    "                    \n",
    "                    # Convert languages field from list to string if needed\n",
    "                    wrapped[\"metadata\"] = convert_languages_field_in_metadata(wrapped.get(\"metadata\", {}))\n",
    "                    \n",
    "                    # Replace nulls in wrapped and metadata\n",
    "                    wrapped = replace_nulls(wrapped)\n",
    "                    processed_data.append(wrapped)\n",
    "                else:\n",
    "                    processed_data.append(replace_nulls(item))\n",
    "        else:\n",
    "            item = enumerated_json_data\n",
    "            if isinstance(item, dict):\n",
    "                if \"text\" in item and isinstance(item[\"text\"], str):\n",
    "                    item[\"text\"] = re.sub(r'(?<=[A-Za-z√áƒûƒ∞√ñ≈û√ú√ßƒüƒ±√∂≈ü√º])\\\\- (?=[A-Za-z√áƒûƒ∞√ñ≈û√ú√ßƒüƒ±√∂≈ü√º])', '', item[\"text\"])\n",
    "                \n",
    "                # ENHANCED: Use the new enhanced_wrap_metadata function\n",
    "                wrapped = enhanced_wrap_metadata(item)\n",
    "                wrapped[\"metadata\"] = convert_languages_field_in_metadata(wrapped.get(\"metadata\", {}))\n",
    "                wrapped = replace_nulls(wrapped)\n",
    "                processed_data = [wrapped]\n",
    "            else:\n",
    "                processed_data = [replace_nulls(item)]\n",
    "\n",
    "        # Debug: Check first few items for problematic metadata\n",
    "        if processed_data:\n",
    "            print(\"üîç Checking metadata compatibility...\")\n",
    "            debug_metadata_types(processed_data[:2])\n",
    "\n",
    "        # Add \"title\" field to each dict (existing logic)\n",
    "        position_to_title = {}\n",
    "        last_title_text = None\n",
    "        sorted_data = sorted(processed_data, key=get_position)\n",
    "        \n",
    "        for item in sorted_data:\n",
    "            item_type = get_type(item)\n",
    "            item_text = get_text(item)\n",
    "            item_position = get_position(item)\n",
    "            if item_type == \"Title\" and item_text:\n",
    "                last_title_text = item_text\n",
    "            if item_position is not None:\n",
    "                position_to_title[item_position] = last_title_text\n",
    "\n",
    "        for item in sorted_data:\n",
    "            item_position = get_position(item)\n",
    "            title_val = position_to_title.get(item_position, None)\n",
    "            if isinstance(item, dict):\n",
    "                if \"metadata\" not in item or not isinstance(item[\"metadata\"], dict):\n",
    "                    item[\"metadata\"] = {}\n",
    "                item[\"metadata\"][\"title\"] = title_val if title_val is not None else \" \"\n",
    "\n",
    "        # Merge logic (existing)\n",
    "        merged_data = []\n",
    "        i = 0\n",
    "        while i < len(sorted_data):\n",
    "            current = sorted_data[i]\n",
    "            if (\n",
    "                i + 1 < len(sorted_data)\n",
    "                and can_merge_nodes(current, sorted_data[i + 1])\n",
    "            ):\n",
    "                merged = merge_nodes(current, sorted_data[i + 1])\n",
    "                merged_data.append(merged)\n",
    "                i += 2\n",
    "            else:\n",
    "                merged_data.append(current)\n",
    "                i += 1\n",
    "\n",
    "        print(f\"Number of nodes before split: {len(merged_data)}\")\n",
    "\n",
    "        # Split text and propagate fields\n",
    "        split_json_data = []\n",
    "        for item in merged_data:\n",
    "            if isinstance(item, dict) and \"text\" in item and isinstance(item[\"text\"], str):\n",
    "                splits = text_splitter.split_text(item[\"text\"])\n",
    "                for split_text in splits:\n",
    "                    new_item = dict(item)\n",
    "                    new_item[\"text\"] = split_text\n",
    "                    # FINAL CLEANING: Ensure metadata is clean before saving\n",
    "                    if \"metadata\" in new_item:\n",
    "                        new_item[\"metadata\"] = clean_metadata_for_pinecone(new_item[\"metadata\"])\n",
    "                    new_item = replace_nulls(new_item)\n",
    "                    split_json_data.append(new_item)\n",
    "            else:\n",
    "                # FINAL CLEANING: Ensure metadata is clean before saving\n",
    "                if isinstance(item, dict) and \"metadata\" in item:\n",
    "                    item[\"metadata\"] = clean_metadata_for_pinecone(item[\"metadata\"])\n",
    "                split_json_data.append(replace_nulls(item))\n",
    "\n",
    "        print(f\"Number of nodes after split: {len(split_json_data)}\")\n",
    "\n",
    "        # Final validation before saving\n",
    "        print(\"üîç Final metadata validation...\")\n",
    "        for i, item in enumerate(split_json_data[:2]):  # Check first 2 items\n",
    "            if isinstance(item, dict) and \"metadata\" in item:\n",
    "                for key, value in item[\"metadata\"].items():\n",
    "                    if not isinstance(value, (str, int, float, bool, list)):\n",
    "                        print(f\"‚ö†Ô∏è  Warning: Item {i}, field '{key}' has type {type(value)}: {value}\")\n",
    "\n",
    "        # Save the processed/split JSON\n",
    "        prep_json_path = os.path.join(prep_dir, json_file)\n",
    "        with open(prep_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(split_json_data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"‚úÖ Saved processed JSON to: {prep_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb23814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Path to the prep folder and the target file\n",
    "# prep_dir = os.path.join(os.path.dirname(json_dir.rstrip(\"/\\\\\")), \"okumalar-prep\")\n",
    "# target_filename = \"7) e≈üitsiz demokrasiler.json\"\n",
    "# target_path = os.path.join(prep_dir, target_filename)\n",
    "\n",
    "# # Load the JSON data\n",
    "# with open(target_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# # Extract the 'text' field from each element\n",
    "# texts = [element['text'] for element in data if 'text' in element]\n",
    "\n",
    "# # Compute statistics\n",
    "# lengths = [len(text) for text in texts]\n",
    "# count = len(lengths)\n",
    "# average = sum(lengths) / count if count > 0 else 0\n",
    "# max_len = max(lengths) if lengths else 0\n",
    "# min_len = min(lengths) if lengths else 0\n",
    "# median_len = sorted(lengths)[len(lengths) // 2] if lengths else 0\n",
    "\n",
    "# print(f\"Summary statistics for 'text' field in '{target_filename}':\")\n",
    "# print(f\"Number of elements: {count}\")\n",
    "# print(f\"Average length: {average:.2f} characters\")\n",
    "# print(f\"Median length: {median_len:.2f} characters\")\n",
    "# print(f\"Max length: {max_len} characters\")\n",
    "# print(f\"Min length: {min_len} characters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0494c0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics for 'text' field across all files in 'C:/Users/yigit/Desktop/Enterprises/arayuz-4\\okumalar-prep':\n",
      "Number of elements: 156014\n",
      "Average length: 84.40 characters\n",
      "Median length: 33.00 characters\n",
      "Max length: 300 characters\n",
      "Min length: 1 characters\n"
     ]
    }
   ],
   "source": [
    "# Path to the prep folder\n",
    "prep_dir = os.path.join(os.path.dirname(json_dir.rstrip(\"/\\\\\")), \"okumalar-prep\")\n",
    "\n",
    "all_texts = []\n",
    "\n",
    "# Iterate over all JSON files in the prep directory and collect all 'text' fields\n",
    "for json_file in glob.glob(os.path.join(prep_dir, \"*.json\")):\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    all_texts.extend([element['text'] for element in data if 'text' in element])\n",
    "\n",
    "# Compute statistics across all files\n",
    "lengths = [len(text) for text in all_texts]\n",
    "count = len(lengths)\n",
    "average = sum(lengths) / count if count > 0 else 0\n",
    "max_len = max(lengths) if lengths else 0\n",
    "min_len = min(lengths) if lengths else 0\n",
    "median_len = sorted(lengths)[len(lengths) // 2] if lengths else 0\n",
    "\n",
    "print(f\"Summary statistics for 'text' field across all files in '{prep_dir}':\")\n",
    "print(f\"Number of elements: {count}\")\n",
    "print(f\"Average length: {average:.2f} characters\")\n",
    "print(f\"Median length: {median_len:.2f} characters\")\n",
    "print(f\"Max length: {max_len} characters\")\n",
    "print(f\"Min length: {min_len} characters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee92bad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Iterate over all JSON files in prep_dir\n",
    "pdf_dir = \"C:/Users/yigit/Desktop/Enterprises/arayuz-4/okumalar-pdf/\"\n",
    "json_dir = os.path.join(os.path.dirname(pdf_dir.rstrip(\"/\\\\\")), \"okumalar-json\")\n",
    "prep_dir = os.path.join(os.path.dirname(json_dir.rstrip(\"/\\\\\")), \"okumalar-prep\")\n",
    "\n",
    "all_documents = []\n",
    "for json_path in glob.glob(os.path.join(prep_dir, \"*.json\")):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        split_json_data = json.load(f)\n",
    "    # Convert split_json_data to Document objects\n",
    "    documents = [\n",
    "        Document(\n",
    "            page_content=element['text'],\n",
    "            metadata=element['metadata'] | {'element_id': element['element_id']}\n",
    "        )\n",
    "        for element in split_json_data\n",
    "    ]\n",
    "    all_documents.extend(documents)\n",
    "\n",
    "# 2. Create embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# 3. Upsert all documents into Pinecone vector store\n",
    "vectorstore = PineconeVectorStore.from_documents(\n",
    "    documents=all_documents,\n",
    "    embedding=embeddings,\n",
    "    index_name=index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b233dd",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6e3df0",
   "metadata": {},
   "source": [
    "## T√ºrkiye‚Äôde gen√ßlerin (18-30 ya≈ü arasƒ±) yerel ve genel temsil istatistikleri nelerdir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a09846",
   "metadata": {},
   "source": [
    "### Search Kwargs: k=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af22049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make sure you have the correct index_name and embedding model\n",
    "# embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "# vectorstore = PineconeVectorStore(\n",
    "#     index_name=index_name,\n",
    "#     embedding=embeddings\n",
    "# )\n",
    "\n",
    "# metadata_field_info = [\n",
    "#     AttributeInfo(\n",
    "#         name=\"filename\",\n",
    "#         description=\"The name of the PDF document the text comes from\",\n",
    "#         type=\"string\",\n",
    "#     ),\n",
    "#     AttributeInfo(\n",
    "#         name=\"languages\",\n",
    "#         description=\"The list of language codes used in the document (e.g. 'tur', 'eng')\",\n",
    "#         type=\"list[string]\",\n",
    "#     ),\n",
    "#     AttributeInfo(\n",
    "#         name=\"page_number\",\n",
    "#         description=\"The page number within the PDF document\",\n",
    "#         type=\"integer\",\n",
    "#     ),\n",
    "#     AttributeInfo(\n",
    "#         name=\"type\",\n",
    "#         description=\"The structural type of the text chunk (e.g. Title, Paragraph, List, Quote)\",\n",
    "#         type=\"string\",\n",
    "#     ),\n",
    "#     AttributeInfo(\n",
    "#         name=\"title\",\n",
    "#         description=\"The section title of the chunk\",\n",
    "#         type=\"string\",\n",
    "#     ),\n",
    "# ]\n",
    "\n",
    "# document_content_description = \"Sociological and political researches and analyses\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d933aac1",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "562fb1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# retriever = SelfQueryRetriever.from_llm(\n",
    "#     llm=llm,\n",
    "#     vectorstore=vectorstore,\n",
    "#     document_contents=\"text\",\n",
    "#     document_content_description=document_content_description,\n",
    "#     metadata_field_info=metadata_field_info,\n",
    "#     search_kwargs={\"k\": 5}\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d88f19bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Simple prompt ---\n",
    "template = \"\"\"A≈üaƒüƒ±da bazƒ± metin par√ßalarƒ± verilmi≈ütir. Bu bilgilere dayanarak son kullanƒ±cƒ± sorusuna tarafsƒ±z ve g√ºvenli bir ≈üekilde yanƒ±t ver:\n",
    "\n",
    "Metin par√ßalarƒ±:\n",
    "{context}\n",
    "\n",
    "Soru: {question}\n",
    "Yanƒ±t:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58bbe81",
   "metadata": {},
   "source": [
    "### Legacy Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c6d57b",
   "metadata": {},
   "source": [
    "#### GPT 3.5 Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd02bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- QA Chain ---\n",
    "# qa_chain = RetrievalQA.from_chain_type(\n",
    "#     llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
    "#     retriever=vectorstore.as_retriever(search_kwargs={\"k\": 8}),\n",
    "#     chain_type=\"stuff\",\n",
    "#     chain_type_kwargs={\"prompt\": prompt}\n",
    "# )\n",
    "\n",
    "# # --- User question ---\n",
    "# query = \"T√ºrkiye‚Äôde gen√ßlerin (18-30 ya≈ü arasƒ±) yerel ve genel temsil istatistikleri nelerdir?\"\n",
    "\n",
    "# # --- Get Answer ---\n",
    "# result = qa_chain.run(query)\n",
    "# print(\"\\nYanƒ±t:\\n\", result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2416b511",
   "metadata": {},
   "source": [
    "#### GPT 3.5 Turbo (Newer Release)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e08f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- QA Chain ---\n",
    "# qa_chain = RetrievalQA.from_chain_type(\n",
    "#     llm=ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0),\n",
    "#     retriever=vectorstore.as_retriever(search_kwargs={\"k\": 8}),\n",
    "#     chain_type=\"stuff\",\n",
    "#     chain_type_kwargs={\"prompt\": prompt}\n",
    "# )\n",
    "\n",
    "# # --- User question ---\n",
    "# query = \"T√ºrkiye‚Äôde gen√ßlerin (18-30 ya≈ü arasƒ±) yerel ve genel temsil istatistikleri nelerdir?\"\n",
    "\n",
    "# # --- Get Answer ---\n",
    "# result = qa_chain.run(query)\n",
    "# print(\"\\nYanƒ±t:\\n\", result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60acbf31",
   "metadata": {},
   "source": [
    "### Actual Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a255b9",
   "metadata": {},
   "source": [
    "#### GPT 4.1 Mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d28f396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- QA Chain ---\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model=\"gpt-4.1-mini-2025-04-14\", temperature=0),\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 8}),\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# --- User question ---\n",
    "query = \"T√ºrkiye‚Äôde gen√ßlerin (18-30 ya≈ü arasƒ±) yerel ve genel temsil istatistikleri nelerdir?\"\n",
    "\n",
    "# --- Get Answer ---\n",
    "result = qa_chain.run(query)\n",
    "print(\"\\nYanƒ±t:\\n\", result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bf0b3c",
   "metadata": {},
   "source": [
    "#### GPT 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506c18e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- QA Chain ---\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model=\"gpt-4.1-2025-04-14\", temperature=0),\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 8}),\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# --- User question ---\n",
    "query = \"T√ºrkiye‚Äôde gen√ßlerin (18-30 ya≈ü arasƒ±) yerel ve genel temsil istatistikleri nelerdir?\"\n",
    "\n",
    "# --- Get Answer ---\n",
    "result = qa_chain.run(query)\n",
    "print(\"\\nYanƒ±t:\\n\", result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbabdf52",
   "metadata": {},
   "source": [
    "#### GPT 4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b121c08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- QA Chain ---\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model=\"gpt-4o-2024-08-06\", temperature=0),\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 8}),\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# --- User question ---\n",
    "query = \"T√ºrkiye‚Äôde gen√ßlerin (18-30 ya≈ü arasƒ±) yerel ve genel temsil istatistikleri nelerdir?\"\n",
    "\n",
    "# --- Get Answer ---\n",
    "result = qa_chain.run(query)\n",
    "print(\"\\nYanƒ±t:\\n\", result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f353cbe3",
   "metadata": {},
   "source": [
    "### Google Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f173234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from google.generativeai import GenerativeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ff5e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_api_key = os.getenv('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04924498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Gemini API\n",
    "genai.configure(api_key=google_api_key)\n",
    "\n",
    "class GeminiQAChain:\n",
    "    def __init__(self, model_name=\"gemini-2.5-flash\", vectorstore=None, k=4, prompt_template=None):\n",
    "        \"\"\"\n",
    "        Initialize Gemini QA Chain\n",
    "        \n",
    "        Args:\n",
    "            model_name: \"gemini-2.5-pro\" or \"gemini-2.5-flash\"\n",
    "            vectorstore: Your vector store (same as before)\n",
    "            k: Number of documents to retrieve\n",
    "            prompt_template: Custom prompt template\n",
    "        \"\"\"\n",
    "        self.model = GenerativeModel(model_name)\n",
    "        self.vectorstore = vectorstore\n",
    "        self.k = k\n",
    "        self.prompt_template = prompt_template or self._default_prompt_template()\n",
    "    \n",
    "    def _default_prompt_template(self):\n",
    "        return \"\"\"\n",
    "        Context: {context}\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Please provide a comprehensive answer based on the context provided above. If the context doesn't contain enough information to answer the question, please state that clearly.\n",
    "        \n",
    "        Answer:\n",
    "        \"\"\"\n",
    "    \n",
    "    def run(self, query):\n",
    "        \"\"\"\n",
    "        Run the QA chain with the given query\n",
    "        \"\"\"\n",
    "        # Retrieve relevant documents\n",
    "        if self.vectorstore:\n",
    "            retriever = self.vectorstore.as_retriever(search_kwargs={\"k\": self.k})\n",
    "            docs = retriever.get_relevant_documents(query)\n",
    "            context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "        else:\n",
    "            context = \"No context provided\"\n",
    "        \n",
    "        # Format the prompt\n",
    "        formatted_prompt = self.prompt_template.format(\n",
    "            context=context,\n",
    "            question=query\n",
    "        )\n",
    "        \n",
    "        # Generate response using Gemini\n",
    "        try:\n",
    "            response = self.model.generate_content(\n",
    "                formatted_prompt,\n",
    "                generation_config={\n",
    "                    \"temperature\": 0,\n",
    "                    \"top_p\": 1,\n",
    "                    \"top_k\": 1\n",
    "                }\n",
    "            )\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3173be",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# 2. Instantiate vectorstore without upserting data\n",
    "vectorstore = PineconeVectorStore(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413f6320",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt = \"\"\"\n",
    "Baƒülam: {context}\n",
    "\n",
    "Soru: {question}\n",
    "\n",
    "L√ºtfen yukarƒ±daki baƒülam bilgilerine dayanarak kapsamlƒ± bir yanƒ±t verin. T√ºrk√ße yanƒ±tlayƒ±n.\n",
    "\n",
    "Yanƒ±t:\n",
    "\"\"\"\n",
    "\n",
    "# --- User question ---\n",
    "query = \"T√ºrkiye‚Äôde gen√ßlerin (18-30 ya≈ü arasƒ±) yerel ve genel temsil istatistikleri nelerdir?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aa27ca",
   "metadata": {},
   "source": [
    "#### Gemini 2.5 Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea77e488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Using Gemini 2.5 Pro (more capable, slower)\n",
    "qa_chain_pro = GeminiQAChain(\n",
    "    model_name=\"gemini-2.5-pro\",\n",
    "    vectorstore=vectorstore,\n",
    "    k=4,\n",
    "    prompt_template=custom_prompt\n",
    ")\n",
    "\n",
    "result_pro = qa_chain_pro.run(query)\n",
    "print(\"\\nGemini 2.5 Pro Yanƒ±tƒ±:\\n\", result_pro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19665ed4",
   "metadata": {},
   "source": [
    "#### Gemini 2.5 Flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfc7c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Using Gemini 2.5 Flash (faster, good performance)\n",
    "qa_chain_flash = GeminiQAChain(\n",
    "    model_name=\"gemini-2.5-flash\",\n",
    "    vectorstore=vectorstore,  # Your existing vectorstore\n",
    "    k=4,\n",
    "    prompt_template=custom_prompt\n",
    ")\n",
    "\n",
    "result_flash = qa_chain_flash.run(query)\n",
    "print(\"\\nGemini 2.5 Flash Yanƒ±tƒ±:\\n\", result_flash)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07e63b9",
   "metadata": {},
   "source": [
    "#### Gemini 2.5 Flash-Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fbafe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Using Gemini 2.5 Flash (faster, good performance)\n",
    "qa_chain_flash = GeminiQAChain(\n",
    "    model_name=\"gemini-2.5-flash-lite\",\n",
    "    vectorstore=vectorstore,  # Your existing vectorstore\n",
    "    k=4,\n",
    "    prompt_template=custom_prompt\n",
    ")\n",
    "\n",
    "result_flash = qa_chain_flash.run(query)\n",
    "print(\"\\nGemini 2.5 Flash-Lite Yanƒ±tƒ±:\\n\", result_flash)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508e9859",
   "metadata": {},
   "source": [
    "### DeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089227a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae41dd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure DeepSeek API (uses OpenAI-compatible interface)\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=deepseek_api_key,\n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a52b562",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSeekQAChain:\n",
    "    def __init__(self, model_name=\"deepseek-chat\", vectorstore=None, k=4, prompt_template=None, temperature=0):\n",
    "        \"\"\"\n",
    "        Initialize DeepSeek QA Chain\n",
    "        \n",
    "        Args:\n",
    "            model_name: \"deepseek-chat\" (main model)\n",
    "            vectorstore: Your vector store\n",
    "            k: Number of documents to retrieve\n",
    "            prompt_template: Custom prompt template\n",
    "            temperature: 0 for deterministic, higher for creative\n",
    "        \"\"\"\n",
    "        self.client = client\n",
    "        self.model_name = model_name\n",
    "        self.vectorstore = vectorstore\n",
    "        self.k = k\n",
    "        self.temperature = temperature\n",
    "        self.prompt_template = prompt_template or self._default_prompt_template()\n",
    "    \n",
    "    def _default_prompt_template(self):\n",
    "        return \"\"\"\n",
    "        Context: {context}\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Please provide a comprehensive answer based on the context provided above. If the context doesn't contain enough information to answer the question, please state that clearly.\n",
    "        \n",
    "        Answer:\n",
    "        \"\"\"\n",
    "    \n",
    "    def run(self, query):\n",
    "        \"\"\"\n",
    "        Run the QA chain with the given query\n",
    "        \"\"\"\n",
    "        # Retrieve relevant documents\n",
    "        if self.vectorstore:\n",
    "            retriever = self.vectorstore.as_retriever(search_kwargs={\"k\": self.k})\n",
    "            docs = retriever.get_relevant_documents(query)\n",
    "            context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "        else:\n",
    "            context = \"No context provided\"\n",
    "        \n",
    "        # Format the prompt\n",
    "        formatted_prompt = self.prompt_template.format(\n",
    "            context=context,\n",
    "            question=query\n",
    "        )\n",
    "        \n",
    "        # Generate response using DeepSeek\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": formatted_prompt}\n",
    "                ],\n",
    "                temperature=self.temperature,\n",
    "                max_tokens=2048,\n",
    "                top_p=1,\n",
    "                frequency_penalty=0,\n",
    "                presence_penalty=0,\n",
    "                stream=False\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddf4f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Custom prompt for Turkish content ---\n",
    "custom_prompt = \"\"\"\n",
    "Baƒülam: {context}\n",
    "\n",
    "Soru: {question}\n",
    "\n",
    "L√ºtfen yukarƒ±daki baƒülam bilgilerine dayanarak detaylƒ± ve kapsamlƒ± bir yanƒ±t ver. Yanƒ±tƒ±nƒ±zƒ± T√ºrk√ße olarak yaz ve m√ºmk√ºn olduƒüunca objektif bir ≈üekilde kar≈üƒ±la≈ütƒ±rma yap.\n",
    "\n",
    "Yanƒ±t:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fa2d92",
   "metadata": {},
   "source": [
    "#### DeepSeek Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267bfa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Chat (general purpose, excellent reasoning)\n",
    "qa_chain_chat = DeepSeekQAChain(\n",
    "    model_name=\"deepseek-chat\",\n",
    "    vectorstore=vectorstore,  # Your existing vectorstore\n",
    "    k=4,\n",
    "    temperature=0,\n",
    "    prompt_template=custom_prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7774dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- User question ---\n",
    "query = \"T√ºrkiye‚Äôde gen√ßlerin (18-30 ya≈ü arasƒ±) yerel ve genel temsil istatistikleri nelerdir?\"\n",
    "\n",
    "# --- Get Answer ---\n",
    "# Using DeepSeek Chat\n",
    "result_chat = qa_chain_chat.run(query)\n",
    "print(\"\\nDeepSeek Chat Yanƒ±tƒ±:\\n\", result_chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71279957",
   "metadata": {},
   "source": [
    "## Search Kwargs: MMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5544de78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have the correct index_name and embedding model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectorstore = PineconeVectorStore(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"filename\",\n",
    "        description=\"The name of the PDF document the text comes from\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"languages\",\n",
    "        description=\"The list of language codes used in the document (e.g. 'tur', 'eng')\",\n",
    "        type=\"list[string]\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"page_number\",\n",
    "        description=\"The page number within the PDF document\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"type\",\n",
    "        description=\"The structural type of the text chunk (e.g. Title, Paragraph, List, Quote)\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"title\",\n",
    "        description=\"The section title of the chunk\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "document_content_description = \"Sociological and political researches and analyses\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32315686",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a96d53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# retriever = SelfQueryRetriever.from_llm(\n",
    "#     llm=llm,\n",
    "#     vectorstore=vectorstore,\n",
    "#     document_contents=\"text\",\n",
    "#     document_content_description=document_content_description,\n",
    "#     metadata_field_info=metadata_field_info,\n",
    "#     search_kwargs={\"k\": 5}\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924ddeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Simple prompt ---\n",
    "template = \"\"\"A≈üaƒüƒ±da bazƒ± metin par√ßalarƒ± verilmi≈ütir. Bu bilgilere dayanarak son kullanƒ±cƒ± sorusuna tarafsƒ±z ve g√ºvenli bir ≈üekilde yanƒ±t ver:\n",
    "\n",
    "Metin par√ßalarƒ±:\n",
    "{context}\n",
    "\n",
    "Soru: {question}\n",
    "Yanƒ±t:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8bb685",
   "metadata": {},
   "source": [
    "### Legacy Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac2342f",
   "metadata": {},
   "source": [
    "#### GPT 3.5 Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3691e98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- QA Chain ---\n",
    "# qa_chain = RetrievalQA.from_chain_type(\n",
    "#     llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
    "#     retriever=vectorstore.as_retriever(search_kwargs={\"k\": 8}),\n",
    "#     chain_type=\"stuff\",\n",
    "#     chain_type_kwargs={\"prompt\": prompt}\n",
    "# )\n",
    "\n",
    "# # --- User question ---\n",
    "# query = \"T√ºrkiye‚Äôde gen√ßlerin (18-30 ya≈ü arasƒ±) yerel ve genel temsil istatistikleri nelerdir?\"\n",
    "\n",
    "# # --- Get Answer ---\n",
    "# result = qa_chain.run(query)\n",
    "# print(\"\\nYanƒ±t:\\n\", result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279afba4",
   "metadata": {},
   "source": [
    "#### GPT 3.5 Turbo (Newer Release)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d798f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- QA Chain ---\n",
    "# qa_chain = RetrievalQA.from_chain_type(\n",
    "#     llm=ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0),\n",
    "#     retriever=vectorstore.as_retriever(search_kwargs={\"k\": 8}),\n",
    "#     chain_type=\"stuff\",\n",
    "#     chain_type_kwargs={\"prompt\": prompt}\n",
    "# )\n",
    "\n",
    "# # --- User question ---\n",
    "# query = \"T√ºrkiye‚Äôde gen√ßlerin (18-30 ya≈ü arasƒ±) yerel ve genel temsil istatistikleri nelerdir?\"\n",
    "\n",
    "# # --- Get Answer ---\n",
    "# result = qa_chain.run(query)\n",
    "# print(\"\\nYanƒ±t:\\n\", result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9587b54",
   "metadata": {},
   "source": [
    "### Actual Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b3c56b",
   "metadata": {},
   "source": [
    "#### GPT 4.1 Mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e697fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- QA Chain ---\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model=\"gpt-4.1-mini-2025-04-14\", temperature=0),\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 8}),\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# --- User question ---\n",
    "query = \"T√ºrkiye‚Äôde gen√ßlerin (18-30 ya≈ü arasƒ±) yerel ve genel temsil istatistikleri nelerdir?\"\n",
    "\n",
    "# --- Get Answer ---\n",
    "result = qa_chain.run(query)\n",
    "print(\"\\nYanƒ±t:\\n\", result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7672c0a0",
   "metadata": {},
   "source": [
    "#### GPT 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca1af93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- QA Chain ---\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model=\"gpt-4.1-2025-04-14\", temperature=0),\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 8}),\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# --- User question ---\n",
    "query = \"T√ºrkiye‚Äôde gen√ßlerin (18-30 ya≈ü arasƒ±) yerel ve genel temsil istatistikleri nelerdir?\"\n",
    "\n",
    "# --- Get Answer ---\n",
    "result = qa_chain.run(query)\n",
    "print(\"\\nYanƒ±t:\\n\", result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39adb00a",
   "metadata": {},
   "source": [
    "#### GPT 4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e2b7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- QA Chain ---\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model=\"gpt-4o-2024-08-06\", temperature=0),\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 8}),\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# --- User question ---\n",
    "query = \"T√ºrkiye‚Äôde gen√ßlerin (18-30 ya≈ü arasƒ±) yerel ve genel temsil istatistikleri nelerdir?\"\n",
    "\n",
    "# --- Get Answer ---\n",
    "result = qa_chain.run(query)\n",
    "print(\"\\nYanƒ±t:\\n\", result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edcbba6",
   "metadata": {},
   "source": [
    "### Google Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e5d154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from google.generativeai import GenerativeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2223354d",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_api_key = os.getenv('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313ae385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Gemini API\n",
    "genai.configure(api_key=google_api_key)\n",
    "\n",
    "class GeminiQAChain:\n",
    "    def __init__(self, model_name=\"gemini-2.5-flash\", vectorstore=None, k=4, prompt_template=None):\n",
    "        \"\"\"\n",
    "        Initialize Gemini QA Chain\n",
    "        \n",
    "        Args:\n",
    "            model_name: \"gemini-2.5-pro\" or \"gemini-2.5-flash\"\n",
    "            vectorstore: Your vector store (same as before)\n",
    "            k: Number of documents to retrieve\n",
    "            prompt_template: Custom prompt template\n",
    "        \"\"\"\n",
    "        self.model = GenerativeModel(model_name)\n",
    "        self.vectorstore = vectorstore\n",
    "        self.k = k\n",
    "        self.prompt_template = prompt_template or self._default_prompt_template()\n",
    "    \n",
    "    def _default_prompt_template(self):\n",
    "        return \"\"\"\n",
    "        Context: {context}\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Please provide a comprehensive answer based on the context provided above. If the context doesn't contain enough information to answer the question, please state that clearly.\n",
    "        \n",
    "        Answer:\n",
    "        \"\"\"\n",
    "    \n",
    "    def run(self, query):\n",
    "        \"\"\"\n",
    "        Run the QA chain with the given query\n",
    "        \"\"\"\n",
    "        # Retrieve relevant documents\n",
    "        if self.vectorstore:\n",
    "            retriever = self.vectorstore.as_retriever(search_kwargs={\"k\": self.k})\n",
    "            docs = retriever.get_relevant_documents(query)\n",
    "            context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "        else:\n",
    "            context = \"No context provided\"\n",
    "        \n",
    "        # Format the prompt\n",
    "        formatted_prompt = self.prompt_template.format(\n",
    "            context=context,\n",
    "            question=query\n",
    "        )\n",
    "        \n",
    "        # Generate response using Gemini\n",
    "        try:\n",
    "            response = self.model.generate_content(\n",
    "                formatted_prompt,\n",
    "                generation_config={\n",
    "                    \"temperature\": 0,\n",
    "                    \"top_p\": 1,\n",
    "                    \"top_k\": 1\n",
    "                }\n",
    "            )\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e29bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# 2. Instantiate vectorstore without upserting data\n",
    "vectorstore = PineconeVectorStore(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74c552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt = \"\"\"\n",
    "Baƒülam: {context}\n",
    "\n",
    "Soru: {question}\n",
    "\n",
    "L√ºtfen yukarƒ±daki baƒülam bilgilerine dayanarak kapsamlƒ± bir yanƒ±t verin. T√ºrk√ße yanƒ±tlayƒ±n.\n",
    "\n",
    "Yanƒ±t:\n",
    "\"\"\"\n",
    "\n",
    "# --- User question ---\n",
    "query = \"T√ºrkiye‚Äôde gen√ßlerin (18-30 ya≈ü arasƒ±) yerel ve genel temsil istatistikleri nelerdir?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915f7a0a",
   "metadata": {},
   "source": [
    "#### Gemini 2.5 Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0db3200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Using Gemini 2.5 Pro (more capable, slower)\n",
    "qa_chain_pro = GeminiQAChain(\n",
    "    model_name=\"gemini-2.5-pro\",\n",
    "    vectorstore=vectorstore,\n",
    "    k=4,\n",
    "    prompt_template=custom_prompt\n",
    ")\n",
    "\n",
    "result_pro = qa_chain_pro.run(query)\n",
    "print(\"\\nGemini 2.5 Pro Yanƒ±tƒ±:\\n\", result_pro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd28ff2e",
   "metadata": {},
   "source": [
    "#### Gemini 2.5 Flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f47bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Using Gemini 2.5 Flash (faster, good performance)\n",
    "qa_chain_flash = GeminiQAChain(\n",
    "    model_name=\"gemini-2.5-flash\",\n",
    "    vectorstore=vectorstore,  # Your existing vectorstore\n",
    "    k=4,\n",
    "    prompt_template=custom_prompt\n",
    ")\n",
    "\n",
    "result_flash = qa_chain_flash.run(query)\n",
    "print(\"\\nGemini 2.5 Flash Yanƒ±tƒ±:\\n\", result_flash)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bc9bf1",
   "metadata": {},
   "source": [
    "#### Gemini 2.5 Flash-Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a97163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Using Gemini 2.5 Flash (faster, good performance)\n",
    "qa_chain_flash = GeminiQAChain(\n",
    "    model_name=\"gemini-2.5-flash-lite\",\n",
    "    vectorstore=vectorstore,  # Your existing vectorstore\n",
    "    k=4,\n",
    "    prompt_template=custom_prompt\n",
    ")\n",
    "\n",
    "result_flash = qa_chain_flash.run(query)\n",
    "print(\"\\nGemini 2.5 Flash-Lite Yanƒ±tƒ±:\\n\", result_flash)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb49630",
   "metadata": {},
   "source": [
    "### DeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ccf2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa98fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure DeepSeek API (uses OpenAI-compatible interface)\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=deepseek_api_key,\n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925bc135",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSeekQAChain:\n",
    "    def __init__(self, model_name=\"deepseek-chat\", vectorstore=None, k=4, prompt_template=None, temperature=0):\n",
    "        \"\"\"\n",
    "        Initialize DeepSeek QA Chain\n",
    "        \n",
    "        Args:\n",
    "            model_name: \"deepseek-chat\" (main model)\n",
    "            vectorstore: Your vector store\n",
    "            k: Number of documents to retrieve\n",
    "            prompt_template: Custom prompt template\n",
    "            temperature: 0 for deterministic, higher for creative\n",
    "        \"\"\"\n",
    "        self.client = client\n",
    "        self.model_name = model_name\n",
    "        self.vectorstore = vectorstore\n",
    "        self.k = k\n",
    "        self.temperature = temperature\n",
    "        self.prompt_template = prompt_template or self._default_prompt_template()\n",
    "    \n",
    "    def _default_prompt_template(self):\n",
    "        return \"\"\"\n",
    "        Context: {context}\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Please provide a comprehensive answer based on the context provided above. If the context doesn't contain enough information to answer the question, please state that clearly.\n",
    "        \n",
    "        Answer:\n",
    "        \"\"\"\n",
    "    \n",
    "    def run(self, query):\n",
    "        \"\"\"\n",
    "        Run the QA chain with the given query\n",
    "        \"\"\"\n",
    "        # Retrieve relevant documents\n",
    "        if self.vectorstore:\n",
    "            retriever = self.vectorstore.as_retriever(search_kwargs={\"k\": self.k})\n",
    "            docs = retriever.get_relevant_documents(query)\n",
    "            context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "        else:\n",
    "            context = \"No context provided\"\n",
    "        \n",
    "        # Format the prompt\n",
    "        formatted_prompt = self.prompt_template.format(\n",
    "            context=context,\n",
    "            question=query\n",
    "        )\n",
    "        \n",
    "        # Generate response using DeepSeek\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": formatted_prompt}\n",
    "                ],\n",
    "                temperature=self.temperature,\n",
    "                max_tokens=2048,\n",
    "                top_p=1,\n",
    "                frequency_penalty=0,\n",
    "                presence_penalty=0,\n",
    "                stream=False\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff220bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Custom prompt for Turkish content ---\n",
    "custom_prompt = \"\"\"\n",
    "Baƒülam: {context}\n",
    "\n",
    "Soru: {question}\n",
    "\n",
    "L√ºtfen yukarƒ±daki baƒülam bilgilerine dayanarak detaylƒ± ve kapsamlƒ± bir yanƒ±t ver. Yanƒ±tƒ±nƒ±zƒ± T√ºrk√ße olarak yaz ve m√ºmk√ºn olduƒüunca objektif bir ≈üekilde kar≈üƒ±la≈ütƒ±rma yap.\n",
    "\n",
    "Yanƒ±t:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e78437",
   "metadata": {},
   "source": [
    "#### DeepSeek Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe9a504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Chat (general purpose, excellent reasoning)\n",
    "qa_chain_chat = DeepSeekQAChain(\n",
    "    model_name=\"deepseek-chat\",\n",
    "    vectorstore=vectorstore,  # Your existing vectorstore\n",
    "    k=4,\n",
    "    temperature=0,\n",
    "    prompt_template=custom_prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fc35b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- User question ---\n",
    "query = \"T√ºrkiye‚Äôde gen√ßlerin (18-30 ya≈ü arasƒ±) yerel ve genel temsil istatistikleri nelerdir?\"\n",
    "\n",
    "# --- Get Answer ---\n",
    "# Using DeepSeek Chat\n",
    "result_chat = qa_chain_chat.run(query)\n",
    "print(\"\\nDeepSeek Chat Yanƒ±tƒ±:\\n\", result_chat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
